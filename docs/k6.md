# GitLab Performance Tool - Running the Tests

* [GitLab Performance Tool - Preparing the Environment](environment_prep.md)
* [**GitLab Performance Tool - Running the Tests**](k6.md)

With your [environment now prepared](environment_prep.md), next we'll detail how to configure and run [k6](https://k6.io/) tests against a GitLab environment with the GitLab Performance Tool (gpt).

**Note: Before running any tests with the Tool, the intended GitLab environment should be prepared first. Details on how to do this can be found here: [GitLab Performance Tool - Preparing the Environment](environment_prep.md)**

**Note: These docs are for GPT `v2`. For GPT `v1` please refer to the docs [here](https://gitlab.com/gitlab-org/quality/performance/-/blob/v1-master/README.md).**

[[_TOC_]]

## Tool Requirements

### Software

GPT as well as the GPT Data Generator supports GitLab versions **12.5 and higher**. If you need help testing an older version as part of a migration or upgrade, please contact [support](https://support.gitlab.com/hc/en-us/requests/new).

### Hardware

The minimum hardware requirements for running the Tool are dependent on the intended load target (given as Requests per Second or RPS), the underlying hardware and the complexity of the tests being run.

As a rule of thumb, we recommend that to run the default tests at **200 RPS** the machine running the Tool has approximately **1 CPU Core** and **1 GB RAM** available.

So for example, to run the default tests at the following RPS targets the machine should have these specs available as a minimum:

* 200 RPS - 1 Core CPU, 1 GB RAM
* 500 RPS - 2 Core CPU, 2.5 GB RAM
* 1000 RPS - 4 Core CPU, 5 GB RAM

We recommend running the tool with [Docker](https://www.docker.com/) but the tool can also be run on Linux natively. More details can be found in the [Running the Tests with the Tool](#running-the-tests-with-the-tool) section below.

## Configuring the Tool

Before you can run performance tests with the tool against your GitLab environment you need to configure it. This involves configuring the tool with all of the required info about your environment, the project(s) you intend to test against, any additional tests you wish to run and optionally to run the tests.

There are three key pieces of configuration in the tool - [Environments](../k6/config/environments), [Options](../k6/config/options) and [Tests](../k6/tests). This section details each as follows.

Out of the box all the default k6 tests are configured to run against an environment that has at least one instance of the default Test Project setup (see [GitLab Performance Tool - Preparing the Environment](environment_prep.md) for more info). However, this is configurable and the tool can be configured to run against any project providing it has equivalent data to test (more details in the next section).

### Environments

The first piece of configuration is the [Environment Config File](../k6/config/environments), which should already be prepared as part of the [Preparing the Environment](environment_prep.md##preparing-the-environment-file) steps. Refer back to that section in the docs if this file and the target GitLab environment isn't already prepared.

### Options

The second piece of configuration is the [Options Config File](../k6/config/options). This file will set how the tool will run the tests - e.g. how long to run the tests for, how many users and how much throughput.

The [Options Config Files](../k6/config/options) are themselves native [k6 config files](https://docs.k6.io/docs/options). For this tool, we use them to set options but they can also be used to set any valid k6 options as required for advanced use cases.

As an example, the following is one of our Options Config Files, [`20s_2rps.json`](../k6/config/options/20s_2rps.json), that configures the tests to each run for 20 seconds at a rate of 2 Requests Per Second (RPS):

```json
{
  "stages": [
    { "duration": "5s", "target": 2 },
    { "duration": "10s", "target": 2 },
    { "duration": "5s", "target": 0 }
  ],
  "rps": 2,
  "batchPerHost": 0
}
```

* `stages` - Defines the stages k6 should run the tests with. Sets the duration of each stage and how many users (VUs) to use.
  * It should be noted that each stage will ramp up from the previous, so in this example the scenario is to ramp up from 0 to 2 users over 5 seconds and then maintain 2 users for another 10s. Finally, it ramps down to 0 users in the next 5 seconds.
* `rps` - Sets the maximum Requests per Second that k6 can make in total.
* `batchPerHost` - Sets k6 to allow unlimited requests to the same host in [batch](https://docs.k6.io/docs/batch-requests) calls.

Note that it's best practice to set the number of users (VUs) to the same amount as RPS to ensure the target RPS can be reached.

Options config files typically should be saved to the `k6/config/options` directory although you can save it elsewhere if desired.

#### Selecting which Options file to use

As detailed above the Options file tells the tool how to run the tests. GPT comes with a large selection of out of the box Options files and typically one of these will suit most needs.

Selecting the correct options file will depend on the target GitLab environment size and the RPS it would be expected to handle. As a rule of thumb we've validated that following RPS is used for every 1000 users through our [Reference Architectures](https://docs.gitlab.com/ee/administration/reference_architectures/):

* API: 20 RPS
* Web: 2 RPS
* Git: 2 RPS

In addition to the above we've also found that 60 seconds per test is generally enough to get a good result as well as give the tests enough time to handle slower endpoints.

As such, the following options files are recommended to be used based on your target environment user count:

* 1k - `60s_20rps.json`
* 2k - `60s_40rps.json`
* 3k - `60s_60rps.json`
* 5k - `60s_100rps.json`
* 10k - `60s_200rps.json`
* 25k - `60s_500rps.json`
* 50k - `60s_1000rps.json`

### Tests

Finally we have the third and last piece of configuration - the [k6 test scripts](https://docs.k6.io/docs/running-k6#section-executing-local-scripts). Each file contains a test to run against the environment along with any extra config such as setting thresholds.

To get more detailed information about the current test list you can refer to the [Current Test Details wiki page](https://gitlab.com/gitlab-org/quality/performance/wikis/current-test-details).

Like Options, test files are native [k6 test scripts](https://docs.k6.io/docs/running-k6#section-executing-local-scripts) and all valid k6 features and options can be used here.

With the tool, we provide various curated tests and libraries that are designed to performance test a wide range of GitLab functions. We continue to iterate and release tests also. In each test we set the actions to take (e.g. call an API) along with defining thresholds that determine if the test is a success (e.g. actual RPS should be no less than 20% of the target and no more than 5% of requests made can be failures).

As an example, the following is one of our API Tests, [`api_v4_projects_project.js`](../k6/tests/api/api_v4_projects_project.js), that tests the [GET Single Project API](https://docs.gitlab.com/ee/api/projects.html#get-single-project):

```js
/*global __ENV : true  */
/*
@endpoint: `GET /projects/:id`
@description: [Get single project](https://docs.gitlab.com/ee/api/projects.html#get-single-project)
*/

import http from "k6/http";
import { group } from "k6";
import { Rate } from "k6/metrics";
import { logError, getRpsThresholds, getTtfbThreshold, getLargeProjects, selectRandom } from "../../lib/gpt_k6_modules.js";

export let rpsThresholds = getRpsThresholds()
export let ttfbThreshold = getTtfbThreshold()
export let successRate = new Rate("successful_requests")
export let options = {
  thresholds: {
    "successful_requests": [`rate>${__ENV.SUCCESS_RATE_THRESHOLD}`],
    "http_req_waiting": [`p(90)<${ttfbThreshold}`],
    "http_reqs": [`count>=${rpsThresholds['count']}`]
  }
};

export let projects = getLargeProjects(['name', 'group_path_api']);

export function setup() {
  console.log('')
  console.log(`RPS Threshold: ${rpsThresholds['mean']}/s (${rpsThresholds['count']})`)
  console.log(`TTFB P90 Threshold: ${ttfbThreshold}ms`)
  console.log(`Success Rate Threshold: ${parseFloat(__ENV.SUCCESS_RATE_THRESHOLD)*100}%`)
}

export default function() {
  group("API - Project Overview", function() {
    let project = selectRandom(projects);

    let params = { headers: { "Accept": "application/json", "PRIVATE-TOKEN": `${__ENV.ACCESS_TOKEN}` } };
    let res = http.get(`${__ENV.ENVIRONMENT_URL}/api/v4/projects/${project['group_path_api']}%2F${project['name']}`, params);
    /20(0|1)/.test(res.status) ? successRate.add(true) : (successRate.add(false), logError(res));
  });
}
```

The above script is to test the Projects API, namely to [get the details of a specific project](https://docs.gitlab.com/ee/api/projects.html#get-single-project).

The script does the following:

* Informs `eslint` that global environment variables are to be used.
* Sets some tags describing the test that are used by the tool for filtering tests as well as by GitLab Quality for [reporting](https://gitlab.com/gitlab-org/quality/performance/wikis/current-test-details).
* Imports the various k6 libraries and our own custom modules that we use in the script
* Fails the test if the `ACCESS_TOKEN` environment variable hasn't be set on the machine as this test requires it to authenticate. (more details on the token can be found in the [Running Tests](#running-tests) section)
* Configures the Thresholds that will be used to determine if the test has passed or not (more details on the thresholds can be found in the [Test Results](#test-results) section).
* Loads in the Projects defined in the Environment config file that have the `name` and `group` variables defined. If none found the test is skipped and if multiple is found the test will perform runs against each.
* Logs some useful info in the k6 pre function `setup()` that the tool will use for reporting.
* Finally the main test script itself is last. It selects a Project at random, sets the required headers and then calls the endpoint on the Environment. It then checks if the response was valid, adds the result to the threshold and calls our custom module to report any errors once in the test output.

Note that the above is an example of a typical test. Other [tests](https://gitlab.com/gitlab-org/quality/performance/wikis/current-test-details) may vary in their approach depending on the area being targeted.

#### Test Types

The Tool provides several different types of performance tests that aim to cover the GitLab application. Depending on the type of test you'll see the tool adjust the RPS target accordingly (via adjusting the main RPS target by percentage) as each have a different target throughput based on user patterns.

There are three main types of tests that the Tool provides:

* [`API`](../k6/tests/api) - Tests that target [API](https://docs.gitlab.com/ee/api/) endpoints (RPS target: 100%)
* [`Git`](../k6/tests/git) - Tests that target Git endpoints (RPS target: 10%)
* [`Web`](../k6/tests/web) - Tests that target Web page endpoints (RPS target: 10%)
  * Note that the underlying software this tool is based on, k6, [only tests the response time of the Web page source and doesn't render the page itself](https://docs.k6.io/docs/welcome#section-k6-does-not). This means it won't performance test any HTTP calls found in dynamic scripts, etc... More accurate web page tests could be emulated via [k6's HAR conversion tool](https://docs.k6.io/docs/session-recording-har-support_) but again note this would only measure response time and not rendering time. To measure Rendering time of pages we recommend [sitespeed.io](https://www.sitespeed.io/).

In addition to the above we have experimental [Scenario](../k6/tests/scenarios) tests that aim to represent a complete user scenario (e.g. creating a new issue) and [Quarantined](../k6/tests/quarantined) tests due to some ongoing issue with endpoint or test itself (that don't run unless specifically instructed). More details on the individual tests can be found in the [Test Documentation](test_docs/README.md).

Finally, some specific tests also have individual supporting documentation. These can be found here - [Test Documentation](test_docs/README.md).

#### Unsafe Tests

Some tests provided with the Tool perform [unsafe http requests](https://developer.mozilla.org/en-US/docs/Glossary/safe) (POST, PUT, DELETE or PATCH) and will create data on the environment. As such, these tests are not included by default when running the Tool.

To run these tests the `--unsafe` flag should be used (refer to the Tools help output above).

These tests are typically fine to run against your environment if you're using the recommended `gitlabhq` project or any other project that can be easily removed after testing has completed. It's unrecommended to run these tests against a permanent project or a live GitLab environment.

## Running the Tests with the Tool

When all of the above configuration is in place the tests can be run with the tool. This can be done in one of two ways - With our [Docker image](https://hub.docker.com/r/gitlab/gitlab-performance-tool) (recommended) or natively on a Linux based system.

### Docker (Recommended)

The recommended way to run the Tool is with our Docker image, [gitlab/gitlab-performance-tool](https://hub.docker.com/r/gitlab/gitlab-performance-tool) (alternative mirror: [registry.gitlab.com/gitlab-org/quality/performance/gitlab-performance-tool](https://gitlab.com/gitlab-org/quality/performance/container_registry)), which will be regularly updated with new features and tests. It can also be used in environment that are airgapped or running in other network conditions such as behind a http proxy.

The image will start running the tests when it's called. The full options for running the tool can be seen by getting the help output by running `docker run -it gitlab/gitlab-performance-tool --help`:

```txt
GitLab Performance Tool (GPT) v2.0.6 - Performance test runner for GitLab environments based on k6

Documentation: https://gitlab.com/gitlab-org/quality/performance/blob/master/docs/README.md

Usage: run-k6 [options]
Options:
  -e, --environment=<s>     Name of Environment Config file in environments directory that the test(s) will be run with. Alternative filepath can also be given.
  -o, --options=<s>         Name of Options Config file in options directory that the test(s) will be run with. Alternative filepath can also be given. (Default: 20s_2rps.json)
  -t, --tests=<s+>          Names of Test files or directories to run with. When directory given tests will be recursively added from api, web and git subdirs. (Default: tests)
  -s, --scenarios           Include any tests inside the test directory's scenarios subfolder when true.
  -q, --quarantined         Include any tests inside the test directory's quarantined subfolder when true.
  -x, --excludes=<s+>       List of words used to exclude tests by matching against their names.
  -u, --unsafe              Include any tests that perform unsafe requests (POST, PUT, DELETE, PATCH)
  -i, --influxdb-url=<s>    URL of an Influx DB server where GPT can optionally upload test run statistics.
  -h, --help                Show this help message
  -v, --version             Print version and exit

Environment Variable(s):
  ACCESS_TOKEN             A valid GitLab Personal Access Token for the specified environment. The token should come from a User that has admin access for the project(s) to be tested and have all permissions set. (Default: nil)
  GPT_DEBUG                Shows debug output when set to true. (Default: nil)

Examples:
  Run all Tests with the 60s_200rps Options file against the 10k Environment:
    docker run -it gitlab/gitlab-performance-tool --environment 10k.json --options 60s_200rps.json
  Run all API Tests with the 60s_200rps Options file against the 10k Environment:
    docker run -it gitlab/gitlab-performance-tool --environment 10k.json --options 60s_200rps.json --tests api
  Run a specific Test with the 60s_200rps Options file against the 10k Environment:
    docker run -it gitlab/gitlab-performance-tool --environment 10k.json --options 60s_200rps.json --tests api_v4_groups_projects.js
```

As standard with Docker you can mount several volumes to get your own config \ test files into the container and results out. The image provides several specific mount points for you to add your own files (in addition to the default ones) as detailed below:

* root (/)
  * `/config` - For any additional config files (each in their own respective subfolder).
    * `/environments` - [Environment Config files](environment_prep.md##preparing-the-environment-file).
    * `/options` - [Options Config files](../k6/config/options)
    * `/projects` - [Project Config files](environment_prep.md#advanced-setup-with-custom-large-projects) (only required when you're using a custom Large Project).
  * `/results` - Contains any result files after test runs
  * `/tests` - For any additional tests files.

Here's an example of how you would run the Docker image with all pieces of config and results mounted (replacing placeholders as appropriate):

```sh
docker run -it -e ACCESS_TOKEN=<TOKEN> -v <HOST CONFIG FOLDER>:/config -v <HOST TESTS FOLDER>:/tests -v <HOST RESULTS FOLDER>:/results gitlab/gitlab-performance-tool --environment <ENV FILE NAME>.json --options 60s_500rps.json
```

After running the results will be in the folder you mounted. More details on the results can be found in the [Test Output and Results](#test-output-and-results).

See more command examples in the help output by running `docker run -it gitlab/gitlab-performance-tool --help`.

### Linux

You can also run the tool natively on a Linux machine with some caveats:

* The tool has been tested on Debian and Alpine based distros (but it should be able to run on others as well).
* This method will require the machine running the tool to have internet access to install Ruby Gems and k6 (if not already present).

Before running some setup is required for the tool:

1. That [Git LFS](https://git-lfs.github.com/) is installed and any LFS data is confirmed pulled via `git lfs pull`.
1. First, set up [`Ruby`](https://www.ruby-lang.org/en/documentation/installation/) and [`Ruby Bundler`](https://bundler.io) if they aren't already available on the machine.
1. Next, install the required Ruby Gems via Bundler
    * `bundle install`

Next you would need to add any of your custom Environment, Options and Test files to their respective directories. From the tool's root folder this would be `k6/config/environments`, `k6/config/options` and `k6/tests` respectively.

Once setup is done you can run the tool with the `bin/run-k6` script. The options for running the tests are the same as when running in Docker but the examples change to the following:

```txt
Examples:
  Run all Tests with the 60s_200rps Options file against the 10k Environment:
    ./bin/run-k6 --environment 10k.json --options 60s_200rps.json
  Run all API Tests with the 60s_200rps Options file against the 10k Environment:
    ./bin/run-k6 --environment 10k.json --options 60s_200rps.json --tests api
  Run a specific Test with the 60s_200rps Options file against the 10k Environment:
    ./bin/run-k6 --environment 10k.json --options 60s_200rps.json --tests api_v4_groups_projects.js
```

After running the results will be in the tool's `results` folder. More details on the results can be found in [Test Output and Results](#test-output-and-results).

### Location and Network conditions

Two factors that can significantly affect the Tool's results are the conditions it's run in - Namely Location (physically in relation to the GitLab Environment) and Network.

The GPT is designed to test the GitLab application's server performance directly at max throughputs, which in turn involves heavy network use. As such, it's recommended to be run as close as possible physically to the GitLab environment and in optimum network conditions. Through this, the results given by the Tool can be trusted to be representative of the application's actual performance and not disrupted by location or network issues. If this isn't possible an Environment can be configured to specify a latency that the Tool will take into account. See the [Environments](#environments) section for more info.

That said, the GPT could also be utilized to test your location and network conditions as a secondary test. For example, you could run the Tool from a desired location (and network conditions) with a representative throughput (i.e. the amount of users that would be typical for that location). In this example this would be run after doing the primary test as described above then compared to validate that your location and network conditions are also performing as expected.

### Test Output and Results

After starting the tool you will see it running each test in order. Once all tests have completed you will be presented with a results summary. As an example, here is a test summary for all tests done against the `10k` environment:

```txt
* Environment:                10k
* Environment Version:        13.1.0-pre `0813a73d2ef`
* Option:                     60s_200rps
* Date:                       2020-06-09
* Run Time:                   1h 34.28s (Start: 01:29:30 UTC, End: 02:30:05 UTC)
* GPT Version:                v2.0.6

â¯ Overall Results Score: 97.46%

NAME                                                     | RPS   | RPS RESULT           | TTFB AVG   | TTFB P90              | REQ STATUS      | RESULT
---------------------------------------------------------|-------|----------------------|------------|-----------------------|-----------------|-------
api_v4_groups                                            | 200/s | 195.45/s (>160.00/s) | 53.55ms    | 61.45ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_groups_group                                      | 200/s | 195.5/s (>160.00/s)  | 41.06ms    | 45.43ms (<1500ms)     | 100.00% (>95%)  | Passed
api_v4_groups_projects                                   | 200/s | 80.32/s (>32.00/s)   | 2236.65ms  | 2688.18ms (<4500ms)   | 100.00% (>95%)  | Passed
api_v4_projects                                          | 200/s | 78.15/s (>32.00/s)   | 2303.84ms  | 2652.38ms (<7500ms)   | 100.00% (>95%)  | Passed
api_v4_projects_deploy_keys                              | 200/s | 195.98/s (>160.00/s) | 37.53ms    | 41.35ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_projects_issues                                   | 200/s | 190.35/s (>96.00/s)  | 206.23ms   | 278.83ms (<2000ms)    | 100.00% (>95%)  | Passed
api_v4_projects_issues_issue                             | 200/s | 191.58/s (>96.00/s)  | 214.94ms   | 289.81ms (<3000ms)    | 100.00% (>95%)  | Passed
api_v4_projects_languages                                | 200/s | 195.97/s (>160.00/s) | 35.94ms    | 39.37ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests                           | 200/s | 151.78/s (>96.00/s)  | 1179.40ms  | 1488.35ms (<2000ms)   | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests_merge_request             | 200/s | 194.87/s (>160.00/s) | 87.58ms    | 104.82ms (<500ms)     | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests_merge_request_changes     | 200/s | 195.47/s (>160.00/s) | 86.27ms    | 102.34ms (<500ms)     | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests_merge_request_commits     | 200/s | 195.28/s (>160.00/s) | 60.14ms    | 68.36ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests_merge_request_discussions | 200/s | 193.72/s (>160.00/s) | 167.51ms   | 227.62ms (<500ms)     | 100.00% (>95%)  | Passed
api_v4_projects_pagination_keyset                        | 200/s | 79.05/s (>32.00/s)   | 2268.19ms  | 2600.15ms (<7500ms)   | 100.00% (>95%)  | Passed
api_v4_projects_project                                  | 200/s | 195.08/s (>160.00/s) | 92.48ms    | 109.37ms (<500ms)     | 100.00% (>95%)  | Passed
api_v4_projects_project_pipelines                        | 200/s | 195.88/s (>160.00/s) | 47.28ms    | 52.35ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_projects_project_search_blobs                     | 200/s | 191.92/s (>16.00/s)  | 215.00ms   | 289.36ms (<15000ms)   | 100.00% (>95%)  | Passed
api_v4_projects_project_services                         | 200/s | 195.67/s (>160.00/s) | 35.98ms    | 40.18ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_projects_repository_branches                      | 200/s | 84.73/s (>32.00/s)   | 2107.77ms  | 3651.89ms (<7500ms)   | 100.00% (>95%)  | Passed
api_v4_projects_repository_branches_branch               | 200/s | 195.88/s (>160.00/s) | 52.98ms    | 58.60ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_projects_repository_commits                       | 200/s | 195.83/s (>160.00/s) | 54.53ms    | 59.48ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_projects_repository_commits_sha                   | 200/s | 195.73/s (>160.00/s) | 54.65ms    | 60.21ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_projects_repository_commits_sha_diff              | 200/s | 195.88/s (>160.00/s) | 49.58ms    | 53.65ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_projects_repository_compare_commits               | 200/s | 194.58/s (>160.00/s) | 140.13ms   | 153.17ms (<500ms)     | 100.00% (>95%)  | Passed
api_v4_projects_repository_files_file                    | 200/s | 194.32/s (>160.00/s) | 129.48ms   | 173.37ms (<500ms)     | 100.00% (>95%)  | Passed
api_v4_projects_repository_files_file_blame              | 200/s | 9.57/s (>1.60/s)     | 13777.62ms | 18507.23ms (<35000ms) | 100.00% (>95%)  | Passed
api_v4_projects_repository_files_file_raw                | 200/s | 195.12/s (>160.00/s) | 98.35ms    | 113.12ms (<500ms)     | 100.00% (>95%)  | Passed
api_v4_projects_repository_tree                          | 200/s | 195.47/s (>160.00/s) | 82.20ms    | 95.32ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_search_global                                     | 200/s | 168.6/s (>48.00/s)   | 4457.68ms  | 6023.56ms (<25000ms)  | 100.00% (>9.5%) | Passed
api_v4_search_groups                                     | 200/s | 102.28/s (>48.00/s)  | 8262.23ms  | 10785.72ms (<25000ms) | 100.00% (>9.5%) | Passed
api_v4_search_projects                                   | 200/s | 125.6/s (>48.00/s)   | 5634.75ms  | 7429.43ms (<25000ms)  | 100.00% (>9.5%) | Passed
api_v4_user                                              | 200/s | 196.13/s (>160.00/s) | 26.85ms    | 29.93ms (<500ms)      | 100.00% (>95%)  | Passed
api_v4_users                                             | 200/s | 195.57/s (>160.00/s) | 55.05ms    | 63.95ms (<500ms)      | 100.00% (>95%)  | Passed
git_ls_remote                                            | 20/s  | 19.63/s (>16.00/s)   | 43.01ms    | 46.21ms (<500ms)      | 100.00% (>95%)  | Passed
git_pull                                                 | 20/s  | 19.72/s (>16.00/s)   | 49.69ms    | 60.51ms (<500ms)      | 100.00% (>95%)  | Passed
git_push                                                 | 20/s  | 19.35/s (>16.00/s)   | 447.43ms   | 689.75ms (<5000ms)    | 100.00% (>95%)  | Passed
scenario_api_new_branches                                | 2/s   | 2.03/s (>1.60/s)     | 455.70ms   | 466.45ms (<1500ms)    | 100.00% (>95%)  | Passed
scenario_api_new_issues                                  | 2/s   | 2.02/s (>1.60/s)     | 272.03ms   | 671.91ms (<2500ms)    | 100.00% (>95%)  | Passed
web_group                                                | 20/s  | 19.22/s (>16.00/s)   | 110.53ms   | 134.69ms (<500ms)     | 100.00% (>95%)  | Passed
web_project                                              | 20/s  | 19.17/s (>16.00/s)   | 226.66ms   | 287.32ms (<750ms)     | 100.00% (>95%)  | Passed
web_project_branches                                     | 20/s  | 19.0/s (>9.60/s)     | 631.63ms   | 745.81ms (<1500ms)    | 100.00% (>95%)  | Passed
web_project_commits                                      | 20/s  | 19.1/s (>16.00/s)    | 394.51ms   | 420.84ms (<500ms)     | 100.00% (>95%)  | Passed
web_project_file                                         | 20/s  | 11.67/s (>1.60/s)    | 1634.54ms  | 3305.87ms (<5000ms)   | 99.70% (>95%)   | Passed
web_project_file_blame                                   | 20/s  | 3.47/s (>0.80/s)     | 5029.60ms  | 5718.01ms (<20000ms)  | 100.00% (>95%)  | Passed
web_project_files                                        | 20/s  | 17.47/s (>12.00/s)   | 326.42ms   | 393.84ms (<2500ms)    | 100.00% (>95%)  | Passed
web_project_issue                                        | 20/s  | 19.17/s (>16.00/s)   | 331.02ms   | 943.02ms (<1500ms)    | 100.00% (>95%)  | Passed
web_project_issues                                       | 20/s  | 19.33/s (>16.00/s)   | 227.45ms   | 293.01ms (<500ms)     | 100.00% (>95%)  | Passed
web_project_merge_request_changes                        | 20/s  | 13.4/s (>8.00/s)     | 1581.47ms  | 2166.72ms (<4000ms)   | 100.00% (>95%)  | Passed
web_project_merge_request_commits                        | 20/s  | 18.8/s (>9.60/s)     | 461.48ms   | 698.38ms (<1250ms)    | 100.00% (>95%)  | Passed
web_project_merge_request_discussions                    | 20/s  | 18.72/s (>11.20/s)   | 889.14ms   | 2312.06ms (<4000ms)   | 100.00% (>95%)  | Passed
web_project_merge_requests                               | 20/s  | 19.38/s (>16.00/s)   | 247.24ms   | 297.24ms (<500ms)     | 100.00% (>95%)  | Passed
web_project_pipelines                                    | 20/s  | 19.15/s (>9.60/s)    | 289.44ms   | 467.27ms (<1000ms)    | 100.00% (>95%)  | Passed
web_search_global                                        | 20/s  | 19.08/s (>16.00/s)   | 74.77ms    | 165.57ms (<1500ms)    | 100.00% (>95%)  | Passed
web_search_groups                                        | 20/s  | 19.1/s (>16.00/s)    | 68.67ms    | 130.94ms (<1000ms)    | 100.00% (>95%)  | Passed
web_search_projects                                      | 20/s  | 19.13/s (>16.00/s)   | 75.16ms    | 210.09ms (<1000ms)    | 100.00% (>95%)  | Passed
web_user                                                 | 20/s  | 19.28/s (>9.60/s)    | 87.60ms    | 117.30ms (<4000ms)    | 100.00% (>95%)  | Passed
```

Going through the output step by step:

* First there are stats about the environment, tests and the GPT version.
* Next is the Overall Results Score of the environment. This value is calculated from all of the test results and presented as a distilled value to show how well the environment performed overall. Typically a well performing environment should be above 90%.
* After the score is the main results table for each test run. In this table each column shows the following:
  * `NAME` - The name of the test run. Matches the filename of the test as found in the [`tests`](../k6/tests) folder
  * `RPS` - The RPS target used during the test.
  * `RPS RESULT` - The RPS achieved along with it's passing threshold.
  * `TTFB AVG` - The average [Time To First Byte](https://en.wikipedia.org/wiki/Time_to_first_byte) (TTFB) in ms.
  * `TTFB P90` - The 90th [percentile](https://en.wikipedia.org/wiki/Percentile_rank) of TTFB along with it's passing threshold.
  * `REQ STATUS` - The percentage of requests made by the test that returned a successful status (HTTP Code 200 / 201 returned) along with it's passing threshold.
  * `RESULT` - The final result of the test based on it's thresholds.
* Finally the output will end with some optional informational notes for the summary depending on how the results went.

Further information on the results output from `k6` can be found over on it's documentation - [Results output](https://docs.k6.io/docs/results-output).

#### Test Thresholds

As described above there are several thresholds that determine if a test has passed or not:

* RPS achieved by the test is above it's threshold. The threshold is typically the target RPS (with a 20% buffer to account for network or other quirks) by default but tests can have their own different thresholds set depending on the area they are testing. Refer to each test to confirm thresholds.
* [Time To First Byte](https://en.wikipedia.org/wiki/Time_to_first_byte) (TTFB) response time is below it's threshold. Typically this is `500ms` or under for the 90th percentile but like RPS this can differ depending on the test.
* That more than 95% of all requests made were successful (HTTP Code 200 / 201 returned).

#### Evaluating Failures

If any of the tests report RPS, TTFB or Status threshold failures these should be evaluated accordingly in line with the following:

* If any of the tests failed but only by a small amount (e.g. within 10% of the threshold) this is likely due to environmental (such as it still starting up) or network conditions such as latency. If further test runs don't report the same errors then these can typically can be ignored.
* If the failures are substantial (e.g. over 50% of the threshold) this would suggest an environment or product issue and further investigation may be required with even possible escalation through the [appropriate channels](https://about.gitlab.com/support/) (e.g. A support ticket or an issue raised against the main GitLab project).
* If GPT is being run against an older version of GitLab you may see some failures as thresholds are tailored to the latest version.

In some cases you may see tests reporting as failures in the Results Summary but no obvious reason why due to it being a summary. When evaluating failures it's recommended that the full test output is analyzed as well. There you will likely find the specific reason for the failure, such as one specific endpoint failing if there was multiple ones being tested.

#### Comparing Results

We post our own results over on this [project's wiki](https://gitlab.com/gitlab-org/quality/performance/wikis/home) for transparency as well as allowing users to compare.

Currently, you'll find the following results on our Wiki:

* [Latest Results](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/Latest) - Our automated CI pipelines run multiple times each week and will post their result summaries to the wiki here each time.
* [GitLab Versions](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/GitLab-Versions) - A collection of performance test results done against several select release versions of GitLab.

#### Using InfluxDB to store results

GPT provides a custom [InfluxDB writer](https://docs.influxdata.com/influxdb/v1.7/guides/writing_data/#write-data-using-the-influxdb-api) that sends GPT results summary data once the test run is finished:

* Measurements: `ttfb_avg`, `ttfb_p90`, `ttfb_p95`, `rps_result`, `success_rate`, `score`
* Tags: `gitlab_version`, `gitlab_revision`, `gpt_version`, `test_name`

To use [built-in k6 InfluxDB](https://k6.io/docs/getting-started/results-output/influxdb) output pass  `K6_INFLUXDB_OUTPUT` environment variable along with `--influxdb-url` flag to the GPT.

## Troubleshooting

In this section we'll detail any known potential problems when running `k6` and how to manage them.

### `socket: too many open files`

You may see `k6` throw the error `socket: too many open files` many times if you're running a test with particularly high amount of virtual users and throughput, e.g. More than 200 users / RPS.

When this happens it's due to the underlying OS having a limit for how many files can be open at one time, also known as file descriptors. This can be fixed by increasing the number of files that are allowed to be open in the OS. How this is done typically is dependent on the type and version of the OS and you should refer to it's documentation. For example though here are links on how to normally do this on [Linux](https://www.tecmint.com/increase-set-open-file-limits-in-linux/) and [Mac OS](https://medium.com/mindful-technology/too-many-open-files-limit-ulimit-on-mac-os-x-add0f1bfddde) respectively. Another workaround is to run the tests in Docker, which typically have higher limits by default.

### Tests failing due to sign in page redirect

If you're seeing some tests fail, especially web tests, with a redirect error showing in the logs this will likely be due to the target Project not being set as public. The GPT tests all expect the test Project's to have public visibility. After changing the Projects the tests should start to work.
