# GitLab Performance Toolkit - Running Load Performance Tests with `k6`

The Toolkit can be used to test the load performance of any GitLab environment. Building on top of the open source load testing tool [`k6`](https://k6.io/), the Toolkit provides numerous curated tests and configurations that are run with a convenience runner command.

On this page, we'll detail how to setup the Toolkit, how to configure the tests and then how to run them.

**Note: Before running any tests with the Toolkit, the intended GitLab environment should be prepared first. Details on how to do this can be found here: [GitLab Performance Toolkit - Environment Preparation](environment_prep.md)**

* [Toolkit Setup](#toolkit-setup)
* [Test Configuration](#test-configuration)
  * [Environments](#environments)
  * [Scenarios](#scenarios)
  * [Tests](#tests)
* [Running Tests](#running-tests)
  * [Understanding the Results](#understanding-the-results)

## Toolkit Setup

On the machine that will be running the tests the following setup is required:

1. First, set up [`Ruby`](https://www.ruby-lang.org/en/documentation/installation/) and [`Ruby Bundler`](https://bundler.io) if they aren't already available on the machine.
1. Next, install the required Ruby Gems via Bundler
    * `bundle install`

**Note: The runner script will install `k6` to the system temp folder if it's not already installed on the machine or if it's the wrong version**

## Test Configuration

Out of the box all the k6 tests are configured to run against a GitLab environment that has the default Test Project setup (see [GitLab Performance Toolkit - Environment Preparation](environment_prep.md) for more info).

If you are looking to run the tests against your own Environment(s), Test Project(s) or if you want to define new Scenario(s) this section aims to take you through all relevant areas of config and how to set them accordingly.

The k6 tests have a few key areas of configuration - [Environments](../k6/environments), [Scenarios](../k6/scenarios) and [Tests](../k6/tests):

### Environments

The k6 tests require various Environment Variables to be set that detail the Environment and Test Project to be tested. These variables can be set in one of two ways - By setting them in a [Environment Config File](../k6/environments) (recommended) or on the machine itself.

As an example, the following is one of our Environment Config Files - [`10k.json`](../k6/environments/10k.json) - with detailed explanations:

```json
{
  "ENVIRONMENT_NAME": "10k",
  "ENVIRONMENT_URL": "http://10k.testbed.gitlab.net",
  "PROJECT_GROUP": "qa-perf-testing",
  "PROJECT_NAME": "gitlabhq",
  "PROJECT_COMMIT_SHA": "0a99e022",
  "PROJECT_BRANCH": "10-0-stable",
  "PROJECT_FILE_PATH": "qa%2fqa%2erb",
  "PROJECT_MR_COMMITS_IID": "10495",
  "PROJECT_MR_NOTES_IID": "6946",
  "PROJECT_SIGNED_COMMIT_SHA": "6526e91f"
}
```
* `ENVIRONMENT_NAME` - The name of the Environment to be tested. This is used by the tests to name results files, etc... 
* `ENVIRONMENT_URL` - The full URL of the Environment to be tested.
* `PROJECT_GROUP` -  The name of the group that contains the intended project.
* `PROJECT_NAME` - The name of intended project.
* `PROJECT_COMMIT_SHA` - The SHA reference of a large commit available in the project. The size of the commit should be tuned to your environment's requirements.
* `PROJECT_BRANCH` - The name of a large branch available in the project. The size of the branch should be tuned to your environment's requirements.
* `PROJECT_FILE_PATH` - The relative path to a normal sized file in your project.
* `PROJECT_MR_COMMITS_IID` - The [iid](https://docs.gitlab.com/ee/api/#id-vs-iid) of a merge request available in the project that has a large number of commits. The size of the MR should be tuned to your environment's requirements.
* `PROJECT_MR_NOTES_IID` - The [iid](https://docs.gitlab.com/ee/api/#id-vs-iid) of a merge request available in the project that has a large number of notes / comments. The size of the MR notes should be tuned to your environment's requirements.
* `PROJECT_SIGNED_COMMIT_SHA` - The SHA reference of a [signed commit](https://docs.gitlab.com/ee/user/project/repository/gpg_signed_commits/) available in the project.

**Note that all of the above variables are required. Additionally, if any are set on the machine they will take precedence**

### Scenarios

The k6 tests also require the Scenario that they will be run with to be set, e.g. how long to run the tests for, how many users and how much throughput. These can be set in one of two ways - By setting them in a [Scenario Config File](../k6/scenarios) (recommended) or in the test scripts themselves.

The [Scenario Config Files](../k6/scenarios) are themselves native [k6 config files](https://docs.k6.io/docs/options). For this tool, we use them to set scenarios but they can also be used to set any valid k6 options as required.

As an example, the following is one of our Scenario Config Files, [`20s_2rps.json`](../k6/scenarios/20s_2rps.json), with detailed explanations:

```json
{
  "stages": [
    { "duration": "5s", "target": 2 },
    { "duration": "15s", "target": 2 }
  ],
  "rps": 2
}
```

* `stages` - Defines the stages k6 should run the tests with. Sets the duration of each stage and how many users (VUs) to use. 
    * It should be noted that each stage will ramp up from the previous, so in this example the scenario is to ramp up from 0 to 2 users over 5 seconds and then maintain 2 users for another 15s.
* `rps` - Sets the maximum Requests per Second that k6 can make in total.

### Tests

Finally we have the k6 test scripts themselves. Each file contains a test to run against the environment along with any extra config such as setting thresholds.

Like Scenarios, these files are native [k6 test scripts](https://docs.k6.io/docs/running-k6#section-executing-local-scripts) and all valid k6 features and options can be used here.

With the tool, we provide various curated tests that are designed to test a wide range of GitLab functions. In each we set the actions to take (e.g. call an API) along with defining thresholds that determine if the test is a success (e.g. no more than 5% of requests made can be failures).

As an example, the following is one of our Tests, [`api_v4_projects_project.js`](../k6/tests/api_v4_projects_project.js), with detailed explanation:

```js
/*global __ENV : true  */

import http from "k6/http";
import { group, fail } from "k6";
import { Rate } from "k6/metrics";
import { logError } from "./modules/log_error.js";

export let successRate = new Rate("successful_requests");
export let options = {
  thresholds: {
    "successful_requests": ["rate>0.95"],
  }
};

if (!__ENV.ACCESS_TOKEN) fail('ACCESS_TOKEN has not be set. Exiting...')

export default function() {
  group("API - Project Overview", function() {
    let params = { headers: { "Accept": "application/json", "PRIVATE-TOKEN": `${__ENV.ACCESS_TOKEN}` } };
    let res = http.get(`${__ENV.ENVIRONMENT_URL}/api/v4/projects/${__ENV.PROJECT_GROUP}%2F${__ENV.PROJECT_NAME}`, params);
    /20(0|1)/.test(res.status) ? successRate.add(true) : successRate.add(false) && logError(res);
  });
}
```

The above script is to test the Projects API, namely to [get the details of a specific project](https://docs.gitlab.com/ee/api/projects.html#get-single-project).

The script does the following:
* Informs `eslint` that global environment variables are to be used
* Imports the various k6 libraries that we use in the script
* Sets up a custom [threshold](https://docs.k6.io/docs/thresholds) that will monitor the rate of successful requests made during the test and mark it as a failure if more than 5% returned a failure
* Also fail the test if the `ACCESS_TOKEN` environment variable hasn't be set on the machine as this test requires it to authenticate. (more details can be found in the next section)
* Next is the main test script itself. It sets the required headers and then calls the Projects API with the relevant [Environment](../k6/environments) config. It then checks if the response was valid, adds the result to the threshold and calls our custom module to report the error once in the test output.

## Running Tests

With the Environment prepared and Tests configured, the tests can now be run with the [`run-k6`](../k6/run-k6) tool.

Below is the help output for the tool and how it can be used to run the tests. In this example we're running the tool from the [`k6`](../k6) folder with relative paths shown:

```
Usage: run-k6 [options]

Runs k6 Test(s) with the given Scenario against the specified Environment.

Options:
  -e, --environment=<s>    Environment Config file path that contains the relevant Environment Variables to be passed to the tests.
  -s, --scenario=<s>       Path of Scenario Config file path that the tests should be run with. (Default: ./scenarios/2rps.json)
  -t, --tests=<s+>         Path of Test file or folder paths to run. (Default: ./tests)
  -q, --quarantined        Include any tests inside the ./scenarios/quarantined folder when true.
  -u, --upload-results     Upload test results to a InfluxDB server. Requires INFLUXDB_URL Environment Variable to be set.
  -h, --help               Show this help message

Environment Variables:
  ACCESS_TOKEN             A valid GitLab Personal Access Token for the specified environment that's required by various tests. The token should come from a User that has admin access for
the project(s) to be tested and have API and read_repository permissions. (Default: nil)
  INFLUXDB_URL             URL for a InfluxDB server that k6 will upload results to (creates database named k6db). (Default: nil)

Examples:
  Running all Tests with the 60s_200rps Scenario against the 10k Environment:
    ./run-k6 --environment ./environments/10k.json --scenario ./scenarios/60s_200rps.json
  Run a specific Test with the 20s_2rps Scenario against the onprem Environment:
    ./run-k6 --environment ./environments/onprem.json --scenario ./scenarios/20s_2rps.json --tests ./tests/api_v4_projects.js
```

Taking one of the examples above the output you should see would be as follows:
```
GitLab Performance Toolkit - k6 load test runner

Environment Variable ACCESS_TOKEN has not been set. Various tests require this for authentication and they will be skipped for this run. See command help for more info...
k6 not found or different version detected. Downloading k6 v0.25.1 from https://github.com/loadimpact/k6/releases/download/v0.25.1/k6-v0.25.1-linux64.tar.gz to system temp folder...

Saving all test results to ./results/onprem_20190813_142035
Running k6 test 'api_v4_projects' against environment 'onprem'...

          /\      |‾‾|  /‾‾/  /‾/
     /\  /  \     |  |_/  /  / /
    /  \/    \    |      |  /  ‾‾\
   /          \   |  |‾\  \ | (_) |
  / __________ \  |__|  \__\ \___/ .io

  execution: local--------------------------------------------------]   servertor
     output: json=./results/onprem_20190813_142035/onprem_api_v4_projects_results.json
     script: ./tests/api_v4_projects.js

    duration: -, iterations: -
         vus: 1, max: 2

time="2019-08-13T14:20:36Z" level=info msg=Running i=2 t=980.1635ms-] starting
time="2019-08-13T14:20:37Z" level=info msg=Running i=4 t=1.9793256s
time="2019-08-13T14:20:38Z" level=info msg=Running i=6 t=2.979299s
time="2019-08-13T14:20:39Z" level=info msg=Running i=8 t=3.9791742s
time="2019-08-13T14:20:40Z" level=info msg=Running i=10 t=4.9802404s
time="2019-08-13T14:20:41Z" level=info msg=Running i=11 t=5.9802591s
time="2019-08-13T14:20:42Z" level=info msg=Running i=14 t=6.9802652s
time="2019-08-13T14:20:43Z" level=info msg=Running i=16 t=7.9793578s
time="2019-08-13T14:20:44Z" level=info msg=Running i=18 t=8.9793367s
time="2019-08-13T14:20:45Z" level=info msg=Running i=20 t=9.9793548s
time="2019-08-13T14:20:46Z" level=info msg=Running i=22 t=10.9801938s
time="2019-08-13T14:20:47Z" level=info msg=Running i=24 t=11.9803414s
time="2019-08-13T14:20:48Z" level=info msg=Running i=26 t=12.9793261s
time="2019-08-13T14:20:49Z" level=info msg=Running i=28 t=13.9803008s
time="2019-08-13T14:20:50Z" level=info msg=Running i=30 t=14.979217s
time="2019-08-13T14:20:51Z" level=info msg=Running i=32 t=15.9802288s
time="2019-08-13T14:20:52Z" level=info msg=Running i=34 t=16.9793353s
time="2019-08-13T14:20:53Z" level=info msg=Running i=36 t=17.98026s
time="2019-08-13T14:20:54Z" level=info msg=Running i=38 t=18.9802109s
time="2019-08-13T14:20:55Z" level=info msg=Running i=40 t=19.9802737s
time="2019-08-13T14:20:55Z" level=info msg="Test finished" i=40 t=20.0003266s

    █ API - Projects List

    data_received..............: 170 kB  8.5 kB/s
    data_sent..................: 4.6 kB  228 B/s
    group_duration.............: avg=850.21ms min=231.33ms med=994.25ms max=1231.50ms p(90)=1006.04ms p(95)=1023.42ms
    http_req_blocked...........: avg=17.07ms  min=0.02ms   med=0.02ms   max=360.21ms  p(90)=0.02ms    p(95)=16.12ms
    http_req_connecting........: avg=5.37ms   min=0.00ms   med=0.00ms   max=107.72ms  p(90)=0.00ms    p(95)=5.35ms
    http_req_duration..........: avg=212.13ms min=192.80ms med=200.76ms max=433.14ms  p(90)=231.52ms  p(95)=243.15ms
    http_req_receiving.........: avg=0.14ms   min=0.10ms   med=0.13ms   max=0.44ms    p(90)=0.17ms    p(95)=0.21ms
    http_req_sending...........: avg=0.11ms   min=0.06ms   med=0.11ms   max=0.29ms    p(90)=0.12ms    p(95)=0.13ms
    http_req_tls_handshaking...: avg=11.65ms  min=0.00ms   med=0.00ms   max=251.11ms  p(90)=0.00ms    p(95)=10.74ms
    http_req_waiting...........: avg=211.88ms min=192.51ms med=200.55ms max=432.89ms  p(90)=231.31ms  p(95)=242.91ms
    http_reqs..................: 40      1.999967/s
    iteration_duration.........: avg=850.29ms min=231.50ms med=994.29ms max=1231.53ms p(90)=1006.30ms p(95)=1023.65ms
    iterations.................: 40      1.999967/s
  ✓ successful_requests........: 100.00% ✓ 40  ✗ 0
    vus........................: 2       min=1 max=2
    vus_max....................: 2       min=2 max=2


All k6 tests have finished after 20.68s!

Results summary:

Environment:	onprem
Scenario:	20s_2rps
Version:	401 Unauthorized
NAME            | RESULT | DURATION | P95      | RPS_COUNT | RPS_MEAN
----------------|--------|----------|----------|-----------|-----------
api_v4_projects | Passed | 20.0s    | 243.15ms | 40        | 1.999967/s

Saving results summary to:
./results/onprem_20190813_142035/onprem_aggregated_results.json
./results/onprem_20190813_142035/onprem_aggregated_results.txt
```

### Understanding the Results

> Note: In the future we aim to add automated test result thresholds so the tool itself reports if tests have passed or failed.

The best way currently to understand if a tested Environment is performing as expected is to compare it's results to our own. A range of test results from our own testing can be found over on this [project's wiki](https://gitlab.com/gitlab-org/quality/performance/wikis/home). 

Here you will find selected results from manual test runs, such as for [GitLab Versions](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/GitLab-Versions) and our [Reference Environments](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/Reference-Environments). You'll also find the [latest results from our test pipelines](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/Latest), which are updated automatically on every run.
