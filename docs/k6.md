# GitLab Performance Toolkit - Running Load Performance Tests with `k6`

The Toolkit can be used to test the load performance of any GitLab environment. Building on top of the open source load testing tool [`k6`](https://k6.io/), the Toolkit provides numerous curated tests and configurations that are run with a convenience runner command.

On this page, we'll detail how to setup the Toolkit, how to configure the tests and then how to run them.

**Note: Before running any tests with the Toolkit, the intended GitLab environment should be prepared first. Details on how to do this can be found here: [GitLab Performance Toolkit - Environment Preparation](environment_prep.md)**

* [Toolkit Setup](#toolkit-setup)
* [Test Configuration](#test-configuration)
  * [Environments](#environments)
  * [Scenarios](#scenarios)
  * [Tests](#tests)
* [Running Tests](#running-tests)
* [Test Results](#test-results)
  * [Evaluating Failures](#evaluating-failures)
  * [Comparing Results](#comparing-results)


## Toolkit Setup

On the machine that will be running the tests the following setup is required:

1. First, set up [`Ruby`](https://www.ruby-lang.org/en/documentation/installation/) and [`Ruby Bundler`](https://bundler.io) if they aren't already available on the machine.
1. Next, install the required Ruby Gems via Bundler
    * `bundle install`

**Note: The runner script will install `k6` to the system temp folder if it's not already installed on the machine or if it's the wrong version**

## Test Configuration

Out of the box all the k6 tests are configured to run against a GitLab environment that has the default Test Project setup (see [GitLab Performance Toolkit - Environment Preparation](environment_prep.md) for more info).

If you are looking to run the tests against your own Environment(s), Test Project(s) or if you want to define new Scenario(s) this section aims to take you through all relevant areas of config and how to set them accordingly.

The k6 tests have a few key areas of configuration - [Environments](../k6/environments), [Scenarios](../k6/scenarios) and [Tests](../k6/tests):

### Environments

The k6 tests require various Environment Variables to be set that detail the Environment and Test Project to be tested. These variables can be set in one of two ways - By setting them in a [Environment Config File](../k6/environments) (recommended) or on the machine itself.

As an example, the following is one of our Environment Config Files - [`10k.json`](../k6/environments/10k.json) - with detailed explanations:

```json
{
  "ENVIRONMENT_NAME": "10k",
  "ENVIRONMENT_URL": "http://10k.testbed.gitlab.net",
  "PROJECT_GROUP": "qa-perf-testing",
  "PROJECT_NAME": "gitlabhq",
  "PROJECT_COMMIT_SHA": "0a99e022",
  "PROJECT_BRANCH": "10-0-stable",
  "PROJECT_FILE_PATH": "qa%2fqa%2erb",
  "PROJECT_MR_COMMITS_IID": "10495",
  "PROJECT_MR_NOTES_IID": "6946",
  "PROJECT_SIGNED_COMMIT_SHA": "6526e91f"
}
```
* `ENVIRONMENT_NAME` - The name of the Environment to be tested. This is used by the tests to name results files, etc... 
* `ENVIRONMENT_URL` - The full URL of the Environment to be tested.
* `PROJECT_GROUP` -  The name of the group that contains the intended project.
* `PROJECT_NAME` - The name of intended project.
* `PROJECT_COMMIT_SHA` - The SHA reference of a large commit available in the project. The size of the commit should be tuned to your environment's requirements.
* `PROJECT_BRANCH` - The name of a large branch available in the project. The size of the branch should be tuned to your environment's requirements.
* `PROJECT_FILE_PATH` - The relative path to a normal sized file in your project.
* `PROJECT_MR_COMMITS_IID` - The [iid](https://docs.gitlab.com/ee/api/#id-vs-iid) of a merge request available in the project that has a large number of commits. The size of the MR should be tuned to your environment's requirements.
* `PROJECT_MR_NOTES_IID` - The [iid](https://docs.gitlab.com/ee/api/#id-vs-iid) of a merge request available in the project that has a large number of notes / comments. The size of the MR notes should be tuned to your environment's requirements.
* `PROJECT_SIGNED_COMMIT_SHA` - The SHA reference of a [signed commit](https://docs.gitlab.com/ee/user/project/repository/gpg_signed_commits/) available in the project.

**Note that all of the above variables are required. Additionally, if any are set on the machine they will take precedence**

### Scenarios

The k6 tests also require the Scenario that they will be run with to be set, e.g. how long to run the tests for, how many users and how much throughput. These can be set in one of two ways - By setting them in a [Scenario Config File](../k6/scenarios) (recommended) or in the test scripts themselves.

The [Scenario Config Files](../k6/scenarios) are themselves native [k6 config files](https://docs.k6.io/docs/options). For this tool, we use them to set scenarios but they can also be used to set any valid k6 options as required.

As an example, the following is one of our Scenario Config Files, [`20s_2rps.json`](../k6/scenarios/20s_2rps.json), with detailed explanations:

```json
{
  "stages": [
    { "duration": "5s", "target": 2 },
    { "duration": "15s", "target": 2 }
  ],
  "rps": 2
}
```

* `stages` - Defines the stages k6 should run the tests with. Sets the duration of each stage and how many users (VUs) to use. 
    * It should be noted that each stage will ramp up from the previous, so in this example the scenario is to ramp up from 0 to 2 users over 5 seconds and then maintain 2 users for another 15s.
* `rps` - Sets the maximum Requests per Second that k6 can make in total.

### Tests

Finally we have the k6 test scripts themselves. Each file contains a test to run against the environment along with any extra config such as setting thresholds.

Like Scenarios, these files are native [k6 test scripts](https://docs.k6.io/docs/running-k6#section-executing-local-scripts) and all valid k6 features and options can be used here.

With the tool, we provide various curated tests that are designed to test a wide range of GitLab functions. In each we set the actions to take (e.g. call an API) along with defining thresholds that determine if the test is a success (e.g. actual RPS should be no less than 20% of the target and no more than 5% of requests made can be failures).

As an example, the following is one of our Tests, [`api_v4_projects_project.js`](../k6/tests/api_v4_projects_project.js), with detailed explanation:

```js
/*global __ENV : true  */

import http from "k6/http";
import { group, fail } from "k6";
import { Rate } from "k6/metrics";
import { logError, getRpsThresholds } from "./modules/custom_k6_modules.js";

if (!__ENV.ACCESS_TOKEN) fail('ACCESS_TOKEN has not be set. Exiting...')

export let rpsThresholds = getRpsThresholds()
export let successRate = new Rate("successful_requests");
export let options = {
  thresholds: {
    "successful_requests": ["rate>0.95"],
    "http_reqs": [`count>=${rpsThresholds['count']}`]
  }
};

export function setup() {
  console.log('')
  console.log(`RPS Threshold: ${rpsThresholds['mean']}/s (${rpsThresholds['count']})`)
}

export default function() {
  group("API - Project Overview", function() {
    let params = { headers: { "Accept": "application/json", "PRIVATE-TOKEN": `${__ENV.ACCESS_TOKEN}` } };
    let res = http.get(`${__ENV.ENVIRONMENT_URL}/api/v4/projects/${__ENV.PROJECT_GROUP}%2F${__ENV.PROJECT_NAME}`, params);
    /20(0|1)/.test(res.status) ? successRate.add(true) : successRate.add(false) && logError(res);
  });
}
```

The above script is to test the Projects API, namely to [get the details of a specific project](https://docs.gitlab.com/ee/api/projects.html#get-single-project).

The script does the following:
* Informs `eslint` that global environment variables are to be used
* Imports the various k6 libraries that we use in the script
* Sets up a custom [threshold](https://docs.k6.io/docs/thresholds) that will monitor the rate of successful requests made during the test and mark it as a failure if more than 5% returned a failure
* Also fail the test if the `ACCESS_TOKEN` environment variable hasn't be set on the machine as this test requires it to authenticate. (more details can be found in the next section)
* Next is the main test script itself. It sets the required headers and then calls the Projects API with the relevant [Environment](../k6/environments) config. It then checks if the response was valid, adds the result to the threshold and calls our custom module to report the error once in the test output.

## Running Tests

With the Environment prepared and Tests configured, the tests can now be run with the [`run-k6`](../k6/run-k6) tool.

Below is the help output for the tool and how it can be used to run the tests. In this example we're running the tool from the [`k6`](../k6) folder with relative paths shown:

```
Usage: run-k6 [options]

Runs k6 Test(s) with the given Scenario against the specified Environment.

Options:
  -e, --environment=<s>    Environment Config file path that contains the relevant Environment Variables to be passed to the tests.
  -s, --scenario=<s>       Path of Scenario Config file path that the tests should be run with. (Default: ./scenarios/2rps.json)
  -t, --tests=<s+>         Path of Test file or folder paths to run. (Default: ./tests)
  -q, --quarantined        Include any tests inside the ./scenarios/quarantined folder when true.
  -u, --upload-results     Upload test results to a InfluxDB server. Requires INFLUXDB_URL Environment Variable to be set.
  -h, --help               Show this help message

Environment Variables:
  ACCESS_TOKEN             A valid GitLab Personal Access Token for the specified environment that's required by various tests. The token should come from a User that has admin access for
the project(s) to be tested and have API and read_repository permissions. (Default: nil)
  INFLUXDB_URL             URL for a InfluxDB server that k6 will upload results to (creates database named k6db). (Default: nil)

Examples:
  Running all Tests with the 60s_200rps Scenario against the 10k Environment:
    ./run-k6 --environment ./environments/10k.json --scenario ./scenarios/60s_200rps.json
  Run a specific Test with the 20s_2rps Scenario against the onprem Environment:
    ./run-k6 --environment ./environments/onprem.json --scenario ./scenarios/20s_2rps.json --tests ./tests/api_v4_projects.js
```

Taking one of the examples above the output you should see would be as follows:
```
GitLab Performance Toolkit - k6 load test runner

Saving all test results to ./results/onprem_20190823_132831
Running k6 test 'api_v4_projects_project' against environment 'onprem'...

          /\      |‾‾|  /‾‾/  /‾/
     /\  /  \     |  |_/  /  / /
    /  \/    \    |      |  /  ‾‾\
   /          \   |  |‾\  \ | (_) |
  / __________ \  |__|  \__\ \___/ .io

  execution: local--------------------------------------------------]   servertor
     output: json=./results/onprem_20190823_132831/onprem_api_v4_projects_project_results.json
     script: tests/api_v4_projects_project.js

    duration: -, iterations: -
         vus: 1, max: 2

time="2019-08-23T13:28:32+01:00" level=info-------------------------] starting
time="2019-08-23T13:28:32+01:00" level=info msg="RPS Threshold: 1.60/s (32)"
time="2019-08-23T13:28:33+01:00" level=info msg=Running i=1 t=987.26356ms
time="2019-08-23T13:28:34+01:00" level=info msg=Running i=4 t=1.987234214s
time="2019-08-23T13:28:35+01:00" level=info msg=Running i=6 t=2.987233898s
time="2019-08-23T13:28:36+01:00" level=info msg=Running i=8 t=3.987244841s
time="2019-08-23T13:28:37+01:00" level=info msg=Running i=10 t=4.987257737s
time="2019-08-23T13:28:38+01:00" level=info msg=Running i=11 t=5.98723584s
time="2019-08-23T13:28:39+01:00" level=info msg=Running i=14 t=6.987061536s
time="2019-08-23T13:28:40+01:00" level=info msg=Running i=16 t=7.987256116s
time="2019-08-23T13:28:41+01:00" level=info msg=Running i=18 t=8.987238131s
time="2019-08-23T13:28:42+01:00" level=info msg=Running i=20 t=9.98705982s
time="2019-08-23T13:28:43+01:00" level=info msg=Running i=22 t=10.987219756s
time="2019-08-23T13:28:44+01:00" level=info msg=Running i=24 t=11.987207207s
time="2019-08-23T13:28:45+01:00" level=info msg=Running i=26 t=12.987203713s
time="2019-08-23T13:28:46+01:00" level=info msg=Running i=28 t=13.987261101s
time="2019-08-23T13:28:47+01:00" level=info msg=Running i=30 t=14.987297487s
time="2019-08-23T13:28:48+01:00" level=info msg=Running i=32 t=15.98733662s
time="2019-08-23T13:28:49+01:00" level=info msg=Running i=34 t=16.987238909s
time="2019-08-23T13:28:50+01:00" level=info msg=Running i=36 t=17.987229294s
time="2019-08-23T13:28:51+01:00" level=info msg=Running i=38 t=18.987234761s
time="2019-08-23T13:28:52+01:00" level=info msg=Running i=40 t=19.987253059s
time="2019-08-23T13:28:52+01:00" level=info msg="Test finished" i=40 t=20.000242778s

    █ API - Project Overview

    data_received..............: 120 kB  6.0 kB/s
    data_sent..................: 4.7 kB  235 B/s
    group_duration.............: avg=862.29ms min=310.32ms med=996.18ms max=1295.91ms p(90)=1011.29ms p(95)=1087.49ms
    http_req_blocked...........: avg=19.18ms  min=0.00ms   med=0.00ms   max=447.97ms  p(90)=0.00ms    p(95)=15.96ms
    http_req_connecting........: avg=5.33ms   min=0.00ms   med=0.00ms   max=106.63ms  p(90)=0.00ms    p(95)=5.32ms
    http_req_duration..........: avg=204.85ms min=182.72ms med=191.14ms max=339.27ms  p(90)=279.03ms  p(95)=286.65ms
    http_req_receiving.........: avg=0.09ms   min=0.06ms   med=0.08ms   max=0.14ms    p(90)=0.11ms    p(95)=0.12ms
    http_req_sending...........: avg=0.07ms   min=0.03ms   med=0.06ms   max=0.18ms    p(90)=0.11ms    p(95)=0.13ms
    http_req_tls_handshaking...: avg=13.81ms  min=0.00ms   med=0.00ms   max=340.00ms  p(90)=0.00ms    p(95)=10.63ms
    http_req_waiting...........: avg=204.70ms min=182.55ms med=190.98ms max=338.98ms  p(90)=278.86ms  p(95)=286.51ms
  ✓ http_reqs..................: 40      1.999976/s
    iteration_duration.........: avg=841.28ms min=0.09ms   med=996.10ms max=1295.96ms p(90)=1009.40ms p(95)=1087.06ms
    iterations.................: 40      1.999976/s
  ✓ successful_requests........: 100.00% ✓ 40  ✗ 0
    vus........................: 2       min=1 max=2
    vus_max....................: 2       min=2 max=2


All k6 tests have finished after 20.69s!

Results summary:

Environment:    onprem (12.0.0-pre 30e31e4afb1)
Scenario:       20s_2rps
Date:           2019-08-23
Run Time:       20.69s (Start: 12:28:31 UTC, End: 12:28:52 UTC)

NAME                    | DURATION | P95      | RPS             | RPS_THRESHOLD | RESULT
------------------------|----------|----------|-----------------|---------------|-------
api_v4_projects_project | 20.0s    | 286.65ms | 1.999976/s (40) | 1.60/s (32)   | Passed

Saving results summary to:
./results/onprem_20190823_132831/onprem_aggregated_results.json
./results/onprem_20190823_132831/onprem_aggregated_results.txt
```

## Test Results

Once all tests have completed you'll be presented with a test summary that is also saved in the `results` folder. As an example here is a test summary for tests done against the `onprem` environment:

```
All k6 tests have finished after 455.51s!

Results summary:

Environment:    onprem (12.0.0-pre 30e31e4afb1)
Scenario:       20s_2rps
Date:           2019-08-23
Run Time:       455.51s (Start: 13:42:08 UTC, End: 13:49:43 UTC)

NAME                                                 | DURATION | P95       | RPS             | RPS_THRESHOLD | RESULT
-----------------------------------------------------|----------|-----------|-----------------|---------------|-------
api_v4_groups_projects                               | 20.0s    | 256.94ms  | 1.999977/s (40) | 1.60/s (32)   | Passed
api_v4_projects_deploy_keys                          | 20.0s    | 210.93ms  | 1.999988/s (40) | 1.60/s (32)   | Passed
api_v4_projects_languages                            | 20.0s    | 182.72ms  | 1.999979/s (40) | 1.60/s (32)   | Passed
api_v4_projects_merge_requests                       | 20.0s    | 1418.02ms | 1.649987/s (33) | 0.56/s (11)   | Passed
api_v4_projects_merge_requests_merge_request         | 20.0s    | 351.40ms  | 1.999973/s (40) | 1.60/s (32)   | Passed
api_v4_projects_merge_requests_merge_request_changes | 20.0s    | 285.65ms  | 1.999979/s (40) | 1.60/s (32)   | Passed
api_v4_projects_merge_requests_merge_request_commits | 20.0s    | 4876.79ms | 0.399995/s (8)  | 0.16/s (3)    | Passed
api_v4_projects_merge_requests_merge_request_notes   | 20.0s    | 541.88ms  | 1.949998/s (39) | 1.60/s (32)   | Passed
api_v4_projects_project                              | 20.0s    | 1051.18ms | 1.849975/s (37) | 1.60/s (32)   | Passed
api_v4_projects_project_pipelines                    | 20.0s    | 223.30ms  | 1.99998/s (40)  | 1.60/s (32)   | Passed
api_v4_projects_repository_branches                  | 20.0s    | 1113.88ms | 1.649987/s (33) | 0.24/s (5)    | Passed
api_v4_projects_repository_branches_branch           | 20.0s    | 225.12ms  | 1.999978/s (40) | 1.60/s (32)   | Passed
api_v4_projects_repository_commits                   | 20.0s    | 270.41ms  | 1.999975/s (40) | 1.60/s (32)   | Passed
api_v4_projects_repository_commits_sha               | 20.0s    | 208.05ms  | 1.999979/s (40) | 1.60/s (32)   | Passed
api_v4_projects_repository_commits_sha_diff          | 20.0s    | 192.47ms  | 1.999992/s (40) | 1.60/s (32)   | Passed
api_v4_projects_repository_commits_sha_signature     | 20.0s    | 190.04ms  | 1.999983/s (40) | 1.60/s (32)   | Passed
api_v4_projects_repository_files_file                | 20.0s    | 325.90ms  | 1.949974/s (39) | 1.60/s (32)   | Passed
api_v4_projects_repository_files_file_raw            | 20.0s    | 515.37ms  | 1.899979/s (38) | 1.60/s (32)   | Passed
api_v4_projects_repository_tree                      | 20.0s    | 196.66ms  | 1.999976/s (40) | 1.60/s (32)   | Passed
api_v4_user                                          | 20.0s    | 207.66ms  | 1.999991/s (40) | 1.60/s (32)   | Passed
git_ls_remote                                        | 20.0s    | 448.97ms  | 1.949999/s (39) | 1.60/s (32)   | Passed
projects_blob_controller_show_html                   | 20.0s    | 812.29ms  | 1.749982/s (35) | 1.60/s (32)   | Passed

Saving results summary to:
./results/onprem_20190823_144208/onprem_aggregated_results.json
./results/onprem_20190823_144208/onprem_aggregated_results.txt
```

The tests also have thresholds set to define if a test passes or fails. Currently these thresholds are as follows:

* That the areas being tested are within expected range of the expected RPS (range is 20% to cover network / environment quirks).
* That no more than 5% of requests failed outright.

Note that some tests may have different criteria defined with comments explaining why in each.

### Evaluating Failures

If any of the tests report RPS threshold failures these should be evaluated accordingly in line with the following:

* If any of the tests failed but only by a small amount (e.g. within 10% of the threshold) this is likely due to environmental or network conditions such as latency. As such, these can typically can be ignored if multiple sets of test runs report the same ;level of failures consistently. If seeking confidence the tests can be run again with a slightly lower RPS threshold modifier to confirm.
* If the failures are substantial (e.g. over 50% of the threshold) this would suggest an environment or product issue and further investigation may be required and should be escalated through the [appropriate channels](https://about.gitlab.com/support/) (e.g. A support ticket or an issue raised against the main GitLab project).

If any of the tests report more than 5% of failed requests outright this should be treated the same as a substantial RPS failure above and escalated through similar channels. A common example of this kind of failure is multiple http 500 code errors being thrown by areas that are completely unable to handle the expected throughput and subsequently failing completely.

### Comparing Results

We post our own results over on this [project's wiki](https://gitlab.com/gitlab-org/quality/performance/wikis/home) for transparency as well as allowing users to compare. 

Currently, you'll find the following results on our Wiki:
* [Latest Results](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/Latest) - Our automated CI pipelines run multiple times each week and will post their result summaries to the wiki here each time.
* [GitLab Versions](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/GitLab-Versions) - A collection of performance test results done against several select release versions of GitLab.
