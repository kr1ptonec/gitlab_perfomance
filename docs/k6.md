# GitLab Performance Toolkit - Running Load Performance Tests with `k6`

The Toolkit can be used to test the load performance of any GitLab environment. Building on top of the open source load testing tool [`k6`](https://k6.io/), the Toolkit provides numerous curated tests and configurations that are run with a convenience runner command.

On this page, we'll detail how to setup the Toolkit, how to configure the tests and then how to run them.

**Note: Before running any tests with the Toolkit, the intended GitLab environment should be prepared first. Details on how to do this can be found here: [GitLab Performance Toolkit - Environment Preparation](environment_prep.md)**

* [Toolkit Setup](#toolkit-setup)
* [Test Configuration](#test-configuration)
  * [Environments](#environments)
  * [Scenarios](#scenarios)
  * [Tests](#tests)
* [Running Tests](#running-tests)
  * [Troubleshooting](#troubleshooting)
    * [`socket: too many open files`](#socket-too-many-open-files)
* [Test Results](#test-results)
  * [Evaluating Failures](#evaluating-failures)
  * [Comparing Results](#comparing-results)


## Toolkit Setup

On the machine that will be running the tests the following setup is required:

1. First, set up [`Ruby`](https://www.ruby-lang.org/en/documentation/installation/) and [`Ruby Bundler`](https://bundler.io) if they aren't already available on the machine.
1. Next, install the required Ruby Gems via Bundler
    * `bundle install`

**Note: The runner script will install `k6` to the system temp folder if it's not already installed on the machine or if it's the wrong version**

## Test Configuration

Out of the box all the k6 tests are configured to run against a GitLab environment that has the default Test Project setup (see [GitLab Performance Toolkit - Environment Preparation](environment_prep.md) for more info).

If you are looking to run the tests against your own Environment(s), Test Project(s) or if you want to define new Scenario(s) this section aims to take you through all relevant areas of config and how to set them accordingly.

The k6 tests have a few key areas of configuration - [Environments](../k6/environments), [Scenarios](../k6/scenarios) and [Tests](../k6/tests):

### Environments

The k6 tests require various Environment Variables to be set that detail the Environment and Test Project to be tested. These variables can be set in one of two ways - By setting them in a [Environment Config File](../k6/environments) (recommended) or on the machine itself.

As an example, the following is one of our Environment Config Files - [`10k.json`](../k6/environments/10k.json) - with detailed explanations:

```json
{
  "ENVIRONMENT_NAME": "10k",
  "ENVIRONMENT_URL": "http://10k.testbed.gitlab.net",
  "PROJECT_GROUP": "qa-perf-testing",
  "PROJECT_NAME": "gitlabhq",
  "PROJECT_COMMIT_SHA": "0a99e022",
  "PROJECT_BRANCH": "10-0-stable",
  "PROJECT_FILE_PATH": "qa%2fqa%2erb",
  "PROJECT_MR_COMMITS_IID": "10495",
  "PROJECT_MR_DISCUSSIONS_IID": "6958",
  "PROJECT_SIGNED_COMMIT_SHA": "6526e91f"
}
```
* `ENVIRONMENT_NAME` - The name of the Environment to be tested. This is used by the tests to name results files, etc... 
* `ENVIRONMENT_URL` - The full URL of the Environment to be tested.
* `PROJECT_GROUP` -  The name of the group that contains the intended project.
* `PROJECT_NAME` - The name of intended project.
* `PROJECT_COMMIT_SHA` - The SHA reference of a large commit available in the project. The size of the commit should be tuned to your environment's requirements.
* `PROJECT_BRANCH` - The name of a large branch available in the project. The size of the branch should be tuned to your environment's requirements.
* `PROJECT_FILE_PATH` - The relative path to a normal sized file in your project.
* `PROJECT_MR_COMMITS_IID` - The [iid](https://docs.gitlab.com/ee/api/#id-vs-iid) of a merge request available in the project that has a large number of commits. The size of the MR should be tuned to your environment's requirements.
* `PROJECT_MR_DISCUSSIONS_IID` - The [iid](https://docs.gitlab.com/ee/api/#id-vs-iid) of a merge request available in the project that has a large number of discussions / comments. The size of the MR discussions should be tuned to your environment's requirements.
* `PROJECT_SIGNED_COMMIT_SHA` - The SHA reference of a [signed commit](https://docs.gitlab.com/ee/user/project/repository/gpg_signed_commits/) available in the project.

**Note that all of the above variables are required. Additionally, if any are set on the machine they will take precedence**

### Scenarios

The k6 tests also require the Scenario that they will be run with to be set, e.g. how long to run the tests for, how many users and how much throughput. These can be set in one of two ways - By setting them in a [Scenario Config File](../k6/scenarios) (recommended) or in the test scripts themselves.

The [Scenario Config Files](../k6/scenarios) are themselves native [k6 config files](https://docs.k6.io/docs/options). For this tool, we use them to set scenarios but they can also be used to set any valid k6 options as required.

As an example, the following is one of our Scenario Config Files, [`20s_2rps.json`](../k6/scenarios/20s_2rps.json), with detailed explanations:

```json
{
  "userAgent": "GitlabK6PerformanceTest/1.0",
  "stages": [
    { "duration": "5s", "target": 2 },
    { "duration": "15s", "target": 2 }
  ],
  "rps": 2
}

```

* `userAgent` - A custom User Agent for the tests to use to allow for easier identification in application logs. This is optional and only used for debugging purposes.
* `stages` - Defines the stages k6 should run the tests with. Sets the duration of each stage and how many users (VUs) to use. 
    * It should be noted that each stage will ramp up from the previous, so in this example the scenario is to ramp up from 0 to 2 users over 5 seconds and then maintain 2 users for another 15s.
* `rps` - Sets the maximum Requests per Second that k6 can make in total.

### Tests

Finally we have the k6 test scripts themselves. Each file contains a test to run against the environment along with any extra config such as setting thresholds.

Like Scenarios, these files are native [k6 test scripts](https://docs.k6.io/docs/running-k6#section-executing-local-scripts) and all valid k6 features and options can be used here.

With the tool, we provide various curated tests that are designed to test a wide range of GitLab functions. In each we set the actions to take (e.g. call an API) along with defining thresholds that determine if the test is a success (e.g. actual RPS should be no less than 20% of the target and no more than 5% of requests made can be failures).

As an example, the following is one of our Tests, [`api_v4_projects_project.js`](../k6/tests/api_v4_projects_project.js), with detailed explanation:

```js
/*global __ENV : true  */

import http from "k6/http";
import { group, fail } from "k6";
import { Rate } from "k6/metrics";
import { logError, getRpsThresholds } from "./modules/custom_k6_modules.js";

if (!__ENV.ACCESS_TOKEN) fail('ACCESS_TOKEN has not be set. Exiting...')

export let rpsThresholds = getRpsThresholds()
export let successRate = new Rate("successful_requests");
export let options = {
  thresholds: {
    "successful_requests": [`rate>${__ENV.SUCCESS_RATE_THRESHOLD}`],
    "http_reqs": [`count>=${rpsThresholds['count']}`]
  }
};

export function setup() {
  console.log('')
  console.log(`RPS Threshold: ${rpsThresholds['mean']}/s (${rpsThresholds['count']})`)
  console.log(`Success Rate Threshold: ${parseFloat(__ENV.SUCCESS_RATE_THRESHOLD)*100}%`)
}

export default function() {
  group("API - Project Overview", function() {
    let params = { headers: { "Accept": "application/json", "PRIVATE-TOKEN": `${__ENV.ACCESS_TOKEN}` } };
    let res = http.get(`${__ENV.ENVIRONMENT_URL}/api/v4/projects/${__ENV.PROJECT_GROUP}%2F${__ENV.PROJECT_NAME}`, params);
    /20(0|1)/.test(res.status) ? successRate.add(true) : successRate.add(false) && logError(res);
  });
}
```

The above script is to test the Projects API, namely to [get the details of a specific project](https://docs.gitlab.com/ee/api/projects.html#get-single-project).

The script does the following:
* Informs `eslint` that global environment variables are to be used
* Imports the various k6 libraries that we use in the script
* Sets up a custom [threshold](https://docs.k6.io/docs/thresholds) that will monitor the rate of successful requests made during the test and mark it as a failure if more than 5% returned a failure
* Also fail the test if the `ACCESS_TOKEN` environment variable hasn't be set on the machine as this test requires it to authenticate. (more details can be found in the next section)
* Next is the main test script itself. It sets the required headers and then calls the Projects API with the relevant [Environment](../k6/environments) config. It then checks if the response was valid, adds the result to the threshold and calls our custom module to report the error once in the test output.

## Running Tests

With the Environment prepared and Tests configured, the tests can now be run with the [`run-k6`](../k6/run-k6) tool.

Below is the help output for the tool and how it can be used to run the tests. In this example we're running the tool from the [`k6`](../k6) folder with relative paths shown:

```
Usage: run-k6 [options]

Runs k6 Test(s) with the given Scenario against the specified Environment.

Options:
  -e, --environment=<s>    Environment Config file path that contains the relevant Environment Variables to be passed to the tests.
  -s, --scenario=<s>       Path of Scenario Config file path that the tests should be run with. (Default: ./scenarios/20s_2rps.json)
  -t, --tests=<s+>         Path of Test file or folder paths to run. (Default: ./tests)
  -a, --standalone         Include any tests inside the ./tests/standalone folder when true.
  -q, --quarantined        Include any tests inside the ./tests/quarantined folder when true.
  -u, --upload-results     Optionally upload test results to a InfluxDB server. Requires K6_INFLUXDB_URL Environment Variable to be set.
  -h, --help               Show this help message

Environment Variables:
  ACCESS_TOKEN             A valid GitLab Personal Access Token for the specified environment that's required by various tests. The token should come from a User that has admin access for
the project(s) to be tested and have API and read_repository permissions. (Default: nil)
  K6_INFLUXDB_URL          Optional URL for a InfluxDB server that k6 will upload results to (creates database named k6db). (Default: nil)

Examples:
  Running all Tests with the 60s_200rps Scenario against the 10k Environment:
    ./run-k6 --environment ./environments/10k.json --scenario ./scenarios/60s_200rps.json
  Run a specific Test with the 20s_2rps Scenario against the onprem Environment:
    ./run-k6 --environment ./environments/onprem.json --scenario ./scenarios/20s_2rps.json --tests ./tests/api_v4_groups_projects.js
```

Taking one of the examples above the output you should see would be as follows:
```
GitLab Performance Toolkit - k6 load test runner

Environment Variable ACCESS_TOKEN has not been set. Various tests require this for authentication and they will be skipped for this run. See command help for more info...

Saving all test results to ./results/onprem_20191015_100846
Running k6 test 'api_v4_groups_projects' against environment 'onprem'...

          /\      |‾‾|  /‾‾/  /‾/
     /\  /  \     |  |_/  /  / /
    /  \/    \    |      |  /  ‾‾\
   /          \   |  |‾\  \ | (_) |
  / __________ \  |__|  \__\ \___/ .io

  execution: local--------------------------------------------------]   servertor
     output: -
     script: ./tests/api_v4_groups_projects.js

    duration: -, iterations: -
         vus: 1, max: 2

time="2019-10-15T10:08:47+01:00" level=info-------------------------] starting
time="2019-10-15T10:08:47+01:00" level=info msg="RPS Threshold: 1.60/s (32)"
time="2019-10-15T10:08:47+01:00" level=info msg="Success Rate Threshold: 95%"
time="2019-10-15T10:08:48+01:00" level=info msg=Running i=2 t=983.037779ms
time="2019-10-15T10:08:49+01:00" level=info msg=Running i=3 t=1.9790663s
time="2019-10-15T10:08:50+01:00" level=info msg=Running i=6 t=2.979073496s
time="2019-10-15T10:08:51+01:00" level=info msg=Running i=8 t=3.979106557s
time="2019-10-15T10:08:52+01:00" level=info msg=Running i=10 t=4.980018924s
time="2019-10-15T10:08:53+01:00" level=info msg=Running i=11 t=5.979199384s
time="2019-10-15T10:08:54+01:00" level=info msg=Running i=14 t=6.980244685s
time="2019-10-15T10:08:55+01:00" level=info msg=Running i=16 t=7.979325698s
time="2019-10-15T10:08:56+01:00" level=info msg=Running i=18 t=8.979257621s
time="2019-10-15T10:08:57+01:00" level=info msg=Running i=20 t=9.979094487s
time="2019-10-15T10:08:58+01:00" level=info msg=Running i=22 t=10.981016656s
time="2019-10-15T10:08:59+01:00" level=info msg=Running i=24 t=11.979069918s
time="2019-10-15T10:09:00+01:00" level=info msg=Running i=26 t=12.983025661s
time="2019-10-15T10:09:01+01:00" level=info msg=Running i=28 t=13.981262441s
time="2019-10-15T10:09:02+01:00" level=info msg=Running i=30 t=14.981238616s
time="2019-10-15T10:09:03+01:00" level=info msg=Running i=32 t=15.982108022s
time="2019-10-15T10:09:04+01:00" level=info msg=Running i=34 t=16.982212385s
time="2019-10-15T10:09:05+01:00" level=info msg=Running i=36 t=17.98207044s
time="2019-10-15T10:09:06+01:00" level=info msg=Running i=38 t=18.981287372s
time="2019-10-15T10:09:07+01:00" level=info msg=Running i=40 t=19.983236995s
time="2019-10-15T10:09:07+01:00" level=info msg="Test finished" i=40 t=20.000218583s

    █ API - Group Projects List

    data_received..............: 150 kB  7.5 kB/s
    data_sent..................: 4.5 kB  227 B/s
    group_duration.............: avg=857.54ms min=245.66ms med=996.78ms max=1199.78ms p(90)=1029.59ms p(95)=1040.91ms
    http_req_blocked...........: avg=18.74ms  min=0.00ms   med=0.00ms   max=437.51ms  p(90)=0.00ms    p(95)=15.60ms
    http_req_connecting........: avg=5.18ms   min=0.00ms   med=0.00ms   max=103.82ms  p(90)=0.00ms    p(95)=5.17ms
    http_req_duration..........: avg=213.05ms min=197.66ms med=204.61ms max=297.40ms  p(90)=240.37ms  p(95)=251.52ms
    http_req_receiving.........: avg=0.12ms   min=0.06ms   med=0.09ms   max=1.23ms    p(90)=0.11ms    p(95)=0.13ms
    http_req_sending...........: avg=0.06ms   min=0.04ms   med=0.06ms   max=0.14ms    p(90)=0.07ms    p(95)=0.07ms
    http_req_tls_handshaking...: avg=13.53ms  min=0.00ms   med=0.00ms   max=332.70ms  p(90)=0.00ms    p(95)=10.42ms
    http_req_waiting...........: avg=212.88ms min=197.50ms med=204.49ms max=297.26ms  p(90)=240.10ms  p(95)=251.24ms
  ✓ http_reqs..................: 40      1.999978/s
    iteration_duration.........: avg=836.65ms min=0.18ms   med=996.03ms max=1199.89ms p(90)=1028.72ms p(95)=1039.76ms
    iterations.................: 40      1.999978/s
  ✓ successful_requests........: 100.00% ✓ 40  ✗ 0
    vus........................: 2       min=1 max=2
    vus_max....................: 2       min=2 max=2


All k6 tests have finished after 20.8s!

Results summary:

Environment:    onprem
Scenario:       20s_2rps
Date:           2019-10-15
Run Time:       20.8s (Start: 09:08:46 UTC, End: 09:09:07 UTC)

NAME                   | DURATION | P95      | RPS             | SUCCESS RATE   | RESULT
-----------------------|----------|----------|-----------------|----------------|-------
api_v4_groups_projects | 20.0s    | 251.52ms | 2.0/s (>1.60/s) | 100.00% (>95%) | Passed

Saving results summary to:
./results/onprem_20191015_100846/onprem_aggregated_results.json
./results/onprem_20191015_100846/onprem_aggregated_results.txt
```

### Troubleshooting

In this section we'll detail any known potential problems when running `k6` and how to manage them.

#### `socket: too many open files`

You may see `k6` throw the error `socket: too many open files` many times if you're running a test with particularly high amount of virtual users and throughput, e.g. More than 200 users / RPS.

When this happens it's due to the underlying OS having a limit for how many files can be open at one time, also known as file descriptors. This can be fixed by increasing the number of files that are allowed to be open in the OS. How this is done typically is dependent on the type and version of the OS and you should refer to it's documentation. For example though here are links on how to normally do this on [Linux](https://www.tecmint.com/increase-set-open-file-limits-in-linux/) and [Mac OS](https://medium.com/mindful-technology/too-many-open-files-limit-ulimit-on-mac-os-x-add0f1bfddde) respectively. Another workaround is to run the tests in Docker, which typically have higher limits by default.

## Test Results

Once all tests have completed you'll be presented with a test summary that is also saved in the `results` folder. As an example here is a test summary for tests done against the `10k` environment with detailed explanations after:

```
Environment:    10k (12.3.5-ee 9dbaa740018)
Scenario:       60s_200rps
Date:           2019-10-15
Run Time:       1690.96s (Start: 01:42:14 UTC, End: 02:10:25 UTC)

NAME                                                     | DURATION | P95        | RPS                  | SUCCESS RATE   | RESULT
---------------------------------------------------------|----------|------------|----------------------|----------------|-------
api_v4_groups_group                                      | 60.0s    | 98.94ms    | 193.77/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_groups_projects                                   | 60.0s    | 105.04ms   | 193.32/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_new_issues                                        | 60.0s    | 6924.64ms  | 51.87/s (>16.00/s)   | 100.00% (>95%) | Passed
api_v4_projects_deploy_keys                              | 60.0s    | 42.00ms    | 197.02/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_projects_languages                                | 60.0s    | 36.46ms    | 197.27/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_projects_merge_requests                           | 60.0s    | 3130.71ms  | 98.33/s (>40.00/s)   | 100.00% (>95%) | Passed
api_v4_projects_merge_requests_merge_request             | 60.0s    | 96.76ms    | 193.33/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_projects_merge_requests_merge_request_changes     | 60.0s    | 87.35ms    | 193.88/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_projects_merge_requests_merge_request_commits     | 60.0s    | 12609.19ms | 23.35/s (>16.00/s)   | 100.00% (>95%) | Passed
api_v4_projects_merge_requests_merge_request_discussions | 60.0s    | 2954.34ms  | 113.27/s (>96.00/s)  | 100.00% (>95%) | Passed
api_v4_projects_project                                  | 60.0s    | 115.30ms   | 192.3/s (>160.00/s)  | 100.00% (>95%) | Passed
api_v4_projects_project_pipelines                        | 60.0s    | 68.21ms    | 195.48/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_projects_project_search_blobs                     | 60.0s    | 1307.21ms  | 149.77/s (>80.00/s)  | 100.00% (>95%) | Passed
api_v4_projects_repository_branches_branch               | 60.0s    | 1142.91ms  | 181.77/s (>80.00/s)  | 100.00% (>95%) | Passed
api_v4_projects_repository_commits                       | 60.0s    | 69.20ms    | 195.0/s (>160.00/s)  | 100.00% (>95%) | Passed
api_v4_projects_repository_commits_sha                   | 60.0s    | 60.79ms    | 195.2/s (>160.00/s)  | 100.00% (>95%) | Passed
api_v4_projects_repository_commits_sha_diff              | 60.0s    | 63.12ms    | 195.3/s (>160.00/s)  | 100.00% (>95%) | Passed
api_v4_projects_repository_commits_sha_signature         | 60.0s    | 53.89ms    | 196.35/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_projects_repository_files_file                    | 60.0s    | 91.86ms    | 193.17/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_projects_repository_files_file_raw                | 60.0s    | 91.85ms    | 193.92/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_projects_repository_tree                          | 60.0s    | 62.50ms    | 195.58/s (>160.00/s) | 100.00% (>95%) | Passed
api_v4_user                                              | 60.0s    | 31.58ms    | 197.82/s (>160.00/s) | 100.00% (>95%) | Passed
git_ls_remote                                            | 60.0s    | 70.45ms    | 19.97/s (>16.00/s)   | 100.00% (>95%) | Passed
git_pull                                                 | 60.0s    | 74.93ms    | 19.65/s (>16.00/s)   | 100.00% (>95%) | Passed
projects_blob_controller_show_html                       | 60.0s    | 463.60ms   | 185.38/s (>160.00/s) | 100.00% (>95%) | Passed
```

* `NAME` - The name of the test run. Matches the filename of the test as found in the [`tests`](../k6/tests) folder
* `DURATION` - The duration each test was run for in seconds.
* `P95` - The 95th [percentile](https://en.wikipedia.org/wiki/Percentile_rank) of the response times seen by the test.
* `RPS` - The average Requests per Second the test was able to achieve throughout it's run along with it's passing threshold.
* `SUCCESS RATE` - The percentage of requests made by the test that were successful (HTTP Code 200 / 201 returned) along with it's passing threshold.
* `RESULT` - The final result of the test.

As described above there are two thresholds that determine if the test has passed or not:

* RPS achieved by the test is above it's target threshold. The threshold is typically the target RPS (with a 20% buffer to account for network or other quirks) by default but tests can have their own different thresholds set depending on the area they are testing. Refer to each test to confirm thresholds
* That more than 95% of all requests made were successful (HTTP Code 200 / 201 returned)

More information on the results output from `k6` can be found over on it's documentation - [Results output](https://docs.k6.io/docs/results-output).

### Evaluating Failures

If any of the tests report RPS threshold failures these should be evaluated accordingly in line with the following:

* If any of the tests failed but only by a small amount (e.g. within 10% of the threshold) this is likely due to environmental or network conditions such as latency. As such, these can typically can be ignored if multiple sets of test runs report the same ;level of failures consistently. If seeking confidence the tests can be run again with a slightly lower RPS threshold modifier to confirm.
* If the failures are substantial (e.g. over 50% of the threshold) this would suggest an environment or product issue and further investigation may be required and should be escalated through the [appropriate channels](https://about.gitlab.com/support/) (e.g. A support ticket or an issue raised against the main GitLab project).

If any of the tests report more than 5% of failed requests outright this should be treated the same as a substantial RPS failure above and escalated through similar channels. A common example of this kind of failure is multiple http 500 code errors being thrown by areas that are completely unable to handle the expected throughput and subsequently failing completely.

### Comparing Results

We post our own results over on this [project's wiki](https://gitlab.com/gitlab-org/quality/performance/wikis/home) for transparency as well as allowing users to compare. 

Currently, you'll find the following results on our Wiki:
* [Latest Results](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/Latest) - Our automated CI pipelines run multiple times each week and will post their result summaries to the wiki here each time.
* [GitLab Versions](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/GitLab-Versions) - A collection of performance test results done against several select release versions of GitLab.
