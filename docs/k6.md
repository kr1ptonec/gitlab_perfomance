# GitLab Performance Tool - Running Tests

On this page, we'll detail how to setup the Tool, how to configure the tests and then how to run them.

**Note: Before running any tests with the Tool, the intended GitLab environment should be prepared first. Details on how to do this can be found here: [GitLab Performance Tool - Environment Preparation](environment_prep.md)**

* [Tool Setup](#tool-setup)
* [Test Configuration](#test-configuration)
  * [Environments](#environments)
  * [Scenarios](#scenarios)
  * [Tests](#tests)
* [Running Tests](#running-tests)
  * [Troubleshooting](#troubleshooting)
    * [`socket: too many open files`](#socket-too-many-open-files)
* [Test Results](#test-results)
  * [Evaluating Failures](#evaluating-failures)
  * [Comparing Results](#comparing-results)

## Tool Setup

On the machine that will be running the tests the following setup is required:

1. First, set up [`Ruby`](https://www.ruby-lang.org/en/documentation/installation/) and [`Ruby Bundler`](https://bundler.io) if they aren't already available on the machine.
1. Next, install the required Ruby Gems via Bundler
    * `bundle install`

**Note: The runner script will install `k6` (which this tool is based on) to the system temp folder if it's not already installed on the machine or if it's the wrong version**

## Test Configuration

Out of the box all the k6 tests are configured to run against a GitLab environment that at least has one instance of the default Test Project setup (see [GitLab Performance Tool - Environment Preparation](environment_prep.md) for more info).

If you are looking to run the tests against your own Environment(s), Test Project(s) or if you want to define new Scenario(s) this section aims to take you through all relevant areas of config and how to set them accordingly.

The k6 tests have a few key areas of configuration - [Environments](../k6/environments), [Scenarios](../k6/scenarios) and [Tests](../k6/tests):

### Environments

The k6 tests require configuration to be passed that sets the environment to be tested along with project(s) details that the tests will expect and use accordingly. This is done with an [Environment Config File](../k6/environments) in JSON format.

As an example, the following is one of our Environment Config Files - [`10k.json`](../k6/environments/10k.json) - with detailed explanations:

```json
{
  "environment": {
    "name": "10k",
    "url": "http://10k.testbed.gitlab.net"
  },
  "projects": [
    {
      "name": "gitlabhq",
      "group": "qa-perf-testing",
      "commit_sha": "0a99e022",
      "commit_sha_signed": "6526e91f",
      "branch": "10-0-stable",
      "file_path": "qa%2fqa%2erb",
      "mr_commits_iid": "10495",
      "mr_discussions_iid": "6958"
    },
    {
      "name": "gitlabhq2",
      "group": "qa-perf-testing",
      "commit_sha": "0a99e022",
      "commit_sha_signed": "6526e91f",
      "branch": "10-0-stable",
      "file_path": "qa%2fqa%2erb",
      "mr_commits_iid": "10495",
      "mr_discussions_iid": "6958"
    }
  ]
}
```

* The environment's Name and URL.
    * Note as a convenience these two settings can also be defined as environment variables, `ENVIRONMENT_NAME` and `ENVIRONMENT_URL` respectively, for overriding as required.
* Details for each project that the tests can target. Each project is expected to have the following defined:
    * `name` - Name of the Project.
    * `group` - The name of the Group that contains the intended Project.
    * `commit_sha` - The SHA reference of a large commit available in the project. The size of the commit should be tuned to your environment's requirements.
    * `commit_sha_signed` - The SHA reference of a [signed commit](https://docs.gitlab.com/ee/user/project/repository/gpg_signed_commits/) available in the project.
    * `branch` - The name of a large branch available in the project. The size of the branch should be tuned to your environment's requirements.
    * `file_path` - The relative path to a normal sized file in your project.
    * `mr_commits_iid` - The [iid](https://docs.gitlab.com/ee/api/#id-vs-iid) of a merge request available in the project that has a large number of commits. The size of the MR should be tuned to your environment's requirements.
    * `mr_discussions_iid` - The [iid](https://docs.gitlab.com/ee/api/#id-vs-iid) of a merge request available in the project that has a large number of discussions / comments. The size of the MR discussions should be tuned to your environment's requirements.
  
> Note that all of the above variables are required

### Scenarios

The k6 tests also require the Scenario that they will be run with to be set, e.g. how long to run the tests for, how many users and how much throughput. These can be set in one of two ways - By setting them in a [Scenario Config File](../k6/scenarios) (recommended) or in the test scripts themselves.

The [Scenario Config Files](../k6/scenarios) are themselves native [k6 config files](https://docs.k6.io/docs/options). For this tool, we use them to set scenarios but they can also be used to set any valid k6 options as required.

As an example, the following is one of our Scenario Config Files, [`20s_2rps.json`](../k6/scenarios/20s_2rps.json), with detailed explanations:

```json
{
  "userAgent": "GitlabK6PerformanceTest/1.0",
  "stages": [
    { "duration": "5s", "target": 2 },
    { "duration": "15s", "target": 2 }
  ],
  "rps": 2
}
```

* `userAgent` - A custom User Agent for the tests to use to allow for easier identification in application logs. This is optional and only used for debugging purposes.
* `stages` - Defines the stages k6 should run the tests with. Sets the duration of each stage and how many users (VUs) to use. 
    * It should be noted that each stage will ramp up from the previous, so in this example the scenario is to ramp up from 0 to 2 users over 5 seconds and then maintain 2 users for another 15s.
* `rps` - Sets the maximum Requests per Second that k6 can make in total.

> Note that it's best practice to set the number of users (VUs) to the same amount as RPS.

### Tests

Finally we have the k6 test scripts themselves. Each file contains a test to run against the environment along with any extra config such as setting thresholds.

To get more detailed information about the current test list you can refer to the [Current Test Details wiki page](https://gitlab.com/gitlab-org/quality/performance/wikis/current-test-details).

Like Scenarios, these files are native [k6 test scripts](https://docs.k6.io/docs/running-k6#section-executing-local-scripts) and all valid k6 features and options can be used here.

With the tool, we provide various curated tests that are designed to test a wide range of GitLab functions. In each we set the actions to take (e.g. call an API) along with defining thresholds that determine if the test is a success (e.g. actual RPS should be no less than 20% of the target and no more than 5% of requests made can be failures).

As an example, the following is one of our Tests, [`api_v4_projects_project.js`](../k6/tests/api/api_v4_projects_project.js), with detailed explanation:

```js
/*global __ENV : true  */
/*
@endpoint: `GET /projects/:id`
@description: [Get single project](https://docs.gitlab.com/ee/api/projects.html#get-single-project)
*/

import http from "k6/http";
import { group, fail } from "k6";
import { Rate } from "k6/metrics";
import { logError, getRpsThresholds, getProjects, selectProject } from "../modules/custom_k6_modules.js";

if (!__ENV.ACCESS_TOKEN) fail('ACCESS_TOKEN has not be set. Exiting...')

export let rpsThresholds = getRpsThresholds()
export let successRate = new Rate("successful_requests");
export let options = {
  thresholds: {
    "successful_requests": [`rate>${__ENV.SUCCESS_RATE_THRESHOLD}`],
    "http_reqs": [`count>=${rpsThresholds['count']}`]
  }
};

export let projects = getProjects();

export function setup() {
  console.log('')
  console.log(`RPS Threshold: ${rpsThresholds['mean']}/s (${rpsThresholds['count']})`)
  console.log(`Success Rate Threshold: ${parseFloat(__ENV.SUCCESS_RATE_THRESHOLD)*100}%`)
}

export default function() {
  group("API - Project Overview", function() {
    let project = selectProject(projects);

    let params = { headers: { "Accept": "application/json", "PRIVATE-TOKEN": `${__ENV.ACCESS_TOKEN}` } };
    let res = http.get(`${__ENV.ENVIRONMENT_URL}/api/v4/projects/${project['group']}%2F${project['name']}`, params);
    /20(0|1)/.test(res.status) ? successRate.add(true) : successRate.add(false) && logError(res);
  });
}
```

The above script is to test the Projects API, namely to [get the details of a specific project](https://docs.gitlab.com/ee/api/projects.html#get-single-project).

The script does the following:
* Informs `eslint` that global environment variables are to be used
* Sets some custom labels describing the test that are used by GitLab Quality for [reporting](https://gitlab.com/gitlab-org/quality/performance/wikis/current-test-details).
* Imports the various k6 libraries and our own custom modules that we use in the script
* Fails the test if the `ACCESS_TOKEN` environment variable hasn't be set on the machine as this test requires it to authenticate. (more details on the token can be found in the [Running Tests](#running-tests) section)
* Configures the Thresholds that will be used to determine if the test has passed or not (more details on the thresholds can be found in the [Test Results](#test-results) section)
* Loads in the Projects defined in the Environment config file
* Logs some useful info that the wrapper tool will use for reporting
* Finally the main test script itself is last. It selects a Project at random, sets the required headers and then calls the endpoint on the Environment. It then checks if the response was valid, adds the result to the threshold and calls our custom module to report any errors once in the test output.

> Note that the above is an example of a typical test. Other [tests](https://gitlab.com/gitlab-org/quality/performance/wikis/current-test-details) may vary in their approach depending on the area being targeted.

## Running Tests

With the Environment prepared and Tests configured, the tests can now be run with the [`run-k6`](../k6/run-k6) wrapper tool.

Below is the help output for the tool and how it can be used to run the tests. In this example we're running the tool from the [`k6`](../k6) folder with relative paths shown:

```
Usage: run-k6 [options]

Runs k6 Test(s) with the given Scenario against the specified Environment.

Options:
  -e, --environment=<s>    Environment Config file path that contains the relevant Environment Variables to be passed to the tests.
  -s, --scenario=<s>       Path of Scenario Config file path that the tests should be run with. (Default: ./scenarios/20s_2rps.json)
  -t, --tests=<s+>         Path of Test file or folder paths to run. (Default: ./tests)
  -a, --standalone         Include any tests inside the ./tests/standalone folder when true.
  -q, --quarantined        Include any tests inside the ./tests/quarantined folder when true.
  -u, --upload-results     Optionally upload test results to a InfluxDB server. Requires K6_INFLUXDB_URL Environment Variable to be set.
  -h, --help               Show this help message

Environment Variables:
  ACCESS_TOKEN             A valid GitLab Personal Access Token for the specified environment that's required by various tests. The token should come from a User that has admin access for
the project(s) to be tested and have API and read_repository permissions. (Default: nil)
  K6_INFLUXDB_URL          Optional URL for a InfluxDB server that k6 will upload results to (creates database named k6db). (Default: nil)

Examples:
  Running all Tests with the 60s_200rps Scenario against the 10k Environment:
    ./run-k6 --environment ./environments/10k.json --scenario ./scenarios/60s_200rps.json
  Run a specific Test with the 20s_2rps Scenario against the onprem Environment:
    ./run-k6 --environment ./environments/onprem.json --scenario ./scenarios/20s_2rps.json --tests ./tests/api/api_v4_groups_projects.js
```

Taking one of the examples above the output you should see would be as follows:
```
GitLab Performance Tool - k6 load test runner

Saving all test results to ./results/10k_20191101_143833
Running k6 test 'api_v4_projects_project' against environment '10k'...

          /\      |‾‾|  /‾‾/  /‾/
     /\  /  \     |  |_/  /  / /
    /  \/    \    |      |  /  ‾‾\
   /          \   |  |‾\  \ | (_) |
  / __________ \  |__|  \__\ \___/ .io

  execution: local--------------------------------------------------]   servertor
     output: -
     script: tests/api/api_v4_projects_project.js

    duration: -, iterations: -
         vus: 1, max: 200

time="2019-11-01T14:38:37Z" level=info------------------------------] starting
time="2019-11-01T14:38:37Z" level=info msg="RPS Threshold: 160.00/s (9600)"
time="2019-11-01T14:38:37Z" level=info msg="Success Rate Threshold: 95%"
time="2019-11-01T14:38:38Z" level=info msg=Running i=2 t=987.172083ms
time="2019-11-01T14:38:39Z" level=info msg=Running i=8 t=1.987289862s
time="2019-11-01T14:38:40Z" level=info msg=Running i=22 t=2.987027208s
time="2019-11-01T14:38:41Z" level=info msg=Running i=47 t=3.987208422s
time="2019-11-01T14:38:42Z" level=info msg=Running i=87 t=4.986230819s
time="2019-11-01T14:38:43Z" level=info msg=Running i=170 t=5.987123314s
time="2019-11-01T14:38:44Z" level=info msg=Running i=308 t=6.989033847s
time="2019-11-01T14:38:45Z" level=info msg=Running i=452 t=7.984059968s
time="2019-11-01T14:38:46Z" level=info msg=Running i=616 t=8.986273111s
time="2019-11-01T14:38:47Z" level=info msg=Running i=905 t=9.98624657s
time="2019-11-01T14:38:48Z" level=info msg=Running i=1127 t=10.98424992s
time="2019-11-01T14:38:49Z" level=info msg=Running i=1326 t=11.984220265s
time="2019-11-01T14:38:50Z" level=info msg=Running i=1528 t=12.984211531s
time="2019-11-01T14:38:51Z" level=info msg=Running i=1725 t=13.989013948s
time="2019-11-01T14:38:52Z" level=info msg=Running i=1929 t=14.989238521s
time="2019-11-01T14:38:53Z" level=info msg=Running i=2128 t=15.988014784s
time="2019-11-01T14:38:54Z" level=info msg=Running i=2326 t=16.984042263s
time="2019-11-01T14:38:55Z" level=info msg=Running i=2528 t=17.986235856s
time="2019-11-01T14:38:56Z" level=info msg=Running i=2730 t=18.988029719s
time="2019-11-01T14:38:57Z" level=info msg=Running i=2928 t=19.984211785s
time="2019-11-01T14:38:58Z" level=info msg=Running i=3126 t=20.988088351s
time="2019-11-01T14:38:59Z" level=info msg=Running i=3328 t=21.989043617s
time="2019-11-01T14:39:00Z" level=info msg=Running i=3527 t=22.988240428s
time="2019-11-01T14:39:01Z" level=info msg=Running i=3726 t=23.98907822s
time="2019-11-01T14:39:02Z" level=info msg=Running i=3929 t=24.984024004s
time="2019-11-01T14:39:03Z" level=info msg=Running i=4127 t=25.98424097s
time="2019-11-01T14:39:04Z" level=info msg=Running i=4326 t=26.989016075s
time="2019-11-01T14:39:05Z" level=info msg=Running i=4529 t=27.984258671s
time="2019-11-01T14:39:06Z" level=info msg=Running i=4729 t=28.984204555s
time="2019-11-01T14:39:07Z" level=info msg=Running i=4926 t=29.984192862s
time="2019-11-01T14:39:08Z" level=info msg=Running i=5126 t=30.988241661s
time="2019-11-01T14:39:09Z" level=info msg=Running i=5329 t=31.987200963s
time="2019-11-01T14:39:10Z" level=info msg=Running i=5527 t=32.988156376s
time="2019-11-01T14:39:11Z" level=info msg=Running i=5727 t=33.986230797s
time="2019-11-01T14:39:12Z" level=info msg=Running i=5928 t=34.988143423s
time="2019-11-01T14:39:13Z" level=info msg=Running i=6127 t=35.989055011s
time="2019-11-01T14:39:14Z" level=info msg=Running i=6329 t=36.988088408s
time="2019-11-01T14:39:15Z" level=info msg=Running i=6528 t=37.988125387s
time="2019-11-01T14:39:16Z" level=info msg=Running i=6728 t=38.984222439s
time="2019-11-01T14:39:17Z" level=info msg=Running i=6926 t=39.984229443s
time="2019-11-01T14:39:18Z" level=info msg=Running i=7128 t=40.989025879s
time="2019-11-01T14:39:19Z" level=info msg=Running i=7327 t=41.989085666s
time="2019-11-01T14:39:20Z" level=info msg=Running i=7527 t=42.988226201s
time="2019-11-01T14:39:21Z" level=info msg=Running i=7726 t=43.984072188s
time="2019-11-01T14:39:22Z" level=info msg=Running i=7926 t=44.984257163s
time="2019-11-01T14:39:23Z" level=info msg=Running i=8123 t=45.989039014s
time="2019-11-01T14:39:24Z" level=info msg=Running i=8327 t=46.986173064s
time="2019-11-01T14:39:25Z" level=info msg=Running i=8527 t=47.984039164s
time="2019-11-01T14:39:26Z" level=info msg=Running i=8727 t=48.984028993s
time="2019-11-01T14:39:27Z" level=info msg=Running i=8927 t=49.984078898s
time="2019-11-01T14:39:28Z" level=info msg=Running i=9126 t=50.987200947s
time="2019-11-01T14:39:29Z" level=info msg=Running i=9328 t=51.987226959s
time="2019-11-01T14:39:30Z" level=info msg=Running i=9527 t=52.986266502s
time="2019-11-01T14:39:31Z" level=info msg=Running i=9728 t=53.984169171s
time="2019-11-01T14:39:32Z" level=info msg=Running i=9928 t=54.989133469s
time="2019-11-01T14:39:33Z" level=info msg=Running i=10128 t=55.988031355s
time="2019-11-01T14:39:34Z" level=info msg=Running i=10329 t=56.984211138s
time="2019-11-01T14:39:35Z" level=info msg=Running i=10528 t=57.984201604s
time="2019-11-01T14:39:36Z" level=info msg=Running i=10728 t=58.989097905s
time="2019-11-01T14:39:37Z" level=info msg=Running i=10928 t=59.984102099s
time="2019-11-01T14:39:37Z" level=info msg="Test finished" i=10931 t=1m0.000022124s

    █ API - Project Overview

    data_received..............: 35 MB   577 kB/s
    data_sent..................: 2.1 MB  35 kB/s
    group_duration.............: avg=960.42ms min=182.69ms med=991.86ms max=2219.72ms p(90)=1027.64ms p(95)=1051.12ms
    http_req_blocked...........: avg=1.95ms   min=0.00ms   med=0.00ms   max=124.88ms  p(90)=0.01ms    p(95)=0.01ms
    http_req_connecting........: avg=1.95ms   min=0.00ms   med=0.00ms   max=124.83ms  p(90)=0.00ms    p(95)=0.00ms
    http_req_duration..........: avg=236.18ms min=176.90ms med=203.76ms max=2181.01ms p(90)=275.79ms  p(95)=424.73ms
    http_req_receiving.........: avg=0.31ms   min=0.03ms   med=0.26ms   max=106.27ms  p(90)=0.46ms    p(95)=0.51ms
    http_req_sending...........: avg=0.02ms   min=0.01ms   med=0.01ms   max=2.18ms    p(90)=0.02ms    p(95)=0.03ms
    http_req_tls_handshaking...: avg=0.00ms   min=0.00ms   med=0.00ms   max=0.00ms    p(90)=0.00ms    p(95)=0.00ms
    http_req_waiting...........: avg=235.85ms min=176.64ms med=203.43ms max=2180.52ms p(90)=275.45ms  p(95)=424.64ms
  ✓ http_reqs..................: 10931   182.183266/s
    iteration_duration.........: avg=960.34ms min=0.15ms   med=991.87ms max=2219.73ms p(90)=1027.65ms p(95)=1051.14ms
    iterations.................: 10931   182.183266/s
  ✓ successful_requests........: 100.00% ✓ 10931 ✗ 0
    vus........................: 200     min=4   max=200
    vus_max....................: 200     min=200 max=200

All k6 tests have finished after 63.98s!

Results summary:

Environment:    10k (12.4.1-ee 8953eae82c8)
Scenario:       60s_200rps
Date:           2019-11-01
Run Time:       63.98s (Start: 14:38:33 UTC, End: 14:39:37 UTC)

NAME                    | RPS   | RPS RESULT           | RESPONSE P95 | REQUEST RESULTS | RESULT
------------------------|-------|----------------------|--------------|-----------------|-------
api_v4_projects_project | 200/s | 182.18/s (>160.00/s) | 424.73ms     | 100.00% (>95%)  | Passed

Saving results summary to:
./results/10k_20191101_143833/10k_2019-11-01_14:38:33_results.json
./results/10k_20191101_143833/10k_2019-11-01_14:38:33_results.txt
```

### Troubleshooting

In this section we'll detail any known potential problems when running `k6` and how to manage them.

#### `socket: too many open files`

You may see `k6` throw the error `socket: too many open files` many times if you're running a test with particularly high amount of virtual users and throughput, e.g. More than 200 users / RPS.

When this happens it's due to the underlying OS having a limit for how many files can be open at one time, also known as file descriptors. This can be fixed by increasing the number of files that are allowed to be open in the OS. How this is done typically is dependent on the type and version of the OS and you should refer to it's documentation. For example though here are links on how to normally do this on [Linux](https://www.tecmint.com/increase-set-open-file-limits-in-linux/) and [Mac OS](https://medium.com/mindful-technology/too-many-open-files-limit-ulimit-on-mac-os-x-add0f1bfddde) respectively. Another workaround is to run the tests in Docker, which typically have higher limits by default.

## Test Results

Once all tests have completed you'll be presented with a test summary that is also saved in the `results` folder. As an example here is a test summary for tests done against the `10k` environment with detailed explanations after:

```
Environment:    10k (12.4.1-ee 8953eae82c8)
Scenario:       60s_200rps
Date:           2019-11-01
Run Time:       1655.04s (Start: 02:12:30 UTC, End: 02:40:05 UTC)

NAME                                                     | RPS   | RPS RESULT           | RESPONSE P95 | REQUEST RESULTS | RESULT
---------------------------------------------------------|-------|----------------------|--------------|-----------------|-------
api_v4_groups_group                                      | 200/s | 185.08/s (>160.00/s) | 190.18ms     | 100.00% (>95%)  | Passed
api_v4_groups_projects                                   | 200/s | 189.92/s (>160.00/s) | 190.87ms     | 100.00% (>95%)  | Passed
api_v4_new_issues                                        | 10/s  | 9.22/s (>8.00/s)     | 565.43ms     | 100.00% (>95%)  | Passed
api_v4_projects_deploy_keys                              | 200/s | 195.17/s (>160.00/s) | 66.08ms      | 100.00% (>95%)  | Passed
api_v4_projects_languages                                | 200/s | 195.38/s (>160.00/s) | 49.16ms      | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests                           | 200/s | 62.43/s (>40.00/s)   | 4185.49ms    | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests_merge_request             | 200/s | 191.4/s (>160.00/s)  | 133.26ms     | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests_merge_request_changes     | 200/s | 192.33/s (>160.00/s) | 119.71ms     | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests_merge_request_commits     | 200/s | 22.52/s (>16.00/s)   | 13613.91ms   | 100.00% (>95%)  | Passed
api_v4_projects_merge_requests_merge_request_discussions | 200/s | 113.12/s (>80.00/s)  | 2926.15ms    | 100.00% (>95%)  | Passed
api_v4_projects_project                                  | 200/s | 188.73/s (>160.00/s) | 170.15ms     | 100.00% (>95%)  | Passed
api_v4_projects_project_pipelines                        | 200/s | 193.07/s (>160.00/s) | 102.53ms     | 100.00% (>95%)  | Passed
api_v4_projects_project_search_blobs                     | 200/s | 189.13/s (>80.00/s)  | 172.40ms     | 100.00% (>95%)  | Passed
api_v4_projects_repository_branches_branch               | 200/s | 190.7/s (>80.00/s)   | 131.90ms     | 100.00% (>95%)  | Passed
api_v4_projects_repository_commits                       | 200/s | 194.45/s (>160.00/s) | 84.48ms      | 100.00% (>95%)  | Passed
api_v4_projects_repository_commits_sha                   | 200/s | 194.18/s (>160.00/s) | 82.63ms      | 100.00% (>95%)  | Passed
api_v4_projects_repository_commits_sha_diff              | 200/s | 194.55/s (>160.00/s) | 80.40ms      | 100.00% (>95%)  | Passed
api_v4_projects_repository_commits_sha_signature         | 200/s | 194.52/s (>160.00/s) | 76.67ms      | 100.00% (>95%)  | Passed
api_v4_projects_repository_files_file                    | 200/s | 193.03/s (>160.00/s) | 111.79ms     | 100.00% (>95%)  | Passed
api_v4_projects_repository_files_file_raw                | 200/s | 191.33/s (>160.00/s) | 113.36ms     | 100.00% (>95%)  | Passed
api_v4_projects_repository_tree                          | 200/s | 194.25/s (>160.00/s) | 83.88ms      | 100.00% (>95%)  | Passed
api_v4_user                                              | 200/s | 196.02/s (>160.00/s) | 42.55ms      | 100.00% (>95%)  | Passed
git_ls_remote                                            | 200/s | 19.95/s (>16.00/s)   | 82.52ms      | 100.00% (>95%)  | Passed
git_pull                                                 | 20/s  | 19.48/s (>16.00/s)   | 84.83ms      | 100.00% (>95%)  | Passed
web_projects_blob_controller_show_html                   | 200/s | 183.35/s (>160.00/s) | 594.83ms     | 100.00% (>95%)  | Passed
```

* `NAME` - The name of the test run. Matches the filename of the test as found in the [`tests`](../k6/tests) folder
* `RPS` - The RPS target used during the test.
* `RPS RESULT` - The average Requests per Second the test was able to achieve throughout it's run along with it's passing threshold.
* `RESPONSE P95` - The 95th [percentile](https://en.wikipedia.org/wiki/Percentile_rank) of the response times seen by the test.
* `REQUEST RESULTS` - The percentage of requests made by the test that were successful (HTTP Code 200 / 201 returned) along with it's passing threshold.
* `RESULT` - The final result of the test.

As described above there are two thresholds that determine if the test has passed or not:

* RPS achieved by the test is above it's target threshold. The threshold is typically the target RPS (with a 20% buffer to account for network or other quirks) by default but tests can have their own different thresholds set depending on the area they are testing. Refer to each test to confirm thresholds
* That more than 95% of all requests made were successful (HTTP Code 200 / 201 returned)

More information on the results output from `k6` can be found over on it's documentation - [Results output](https://docs.k6.io/docs/results-output).

### Evaluating Failures

If any of the tests report RPS threshold failures these should be evaluated accordingly in line with the following:

* If any of the tests failed but only by a small amount (e.g. within 10% of the threshold) this is likely due to environmental or network conditions such as latency. As such, these can typically can be ignored if multiple sets of test runs report the same level of failures consistently. If seeking confidence the tests can be run again with a slightly lower RPS threshold modifier to confirm.
* If the failures are substantial (e.g. over 50% of the threshold) this would suggest an environment or product issue and further investigation may be required and should be escalated through the [appropriate channels](https://about.gitlab.com/support/) (e.g. A support ticket or an issue raised against the main GitLab project).

If any of the tests report more than 5% of failed requests outright this should be treated the same as a substantial RPS failure above and escalated through similar channels. A common example of this kind of failure is multiple http 500 code errors being thrown by areas that are completely unable to handle the expected throughput and subsequently failing completely.

### Comparing Results

We post our own results over on this [project's wiki](https://gitlab.com/gitlab-org/quality/performance/wikis/home) for transparency as well as allowing users to compare. 

Currently, you'll find the following results on our Wiki:
* [Latest Results](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/Latest) - Our automated CI pipelines run multiple times each week and will post their result summaries to the wiki here each time.
* [GitLab Versions](https://gitlab.com/gitlab-org/quality/performance/wikis/Benchmarks/GitLab-Versions) - A collection of performance test results done against several select release versions of GitLab.
